<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a> ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a>&nbsp
</p>
<br><br>

<p align="center">
    <img src="assets/logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-VL <a href="https://modelscope.cn/models/qwen/Qwen-VL/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-VL">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat <a href="https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-VL-Chat">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat-Int4 <a href="https://huggingface.co/Qwen/Qwen-VL-Chat-Int4">ğŸ¤—</a>
<br>
<a href="assets/wechat.png">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://arxiv.org/abs/2308.12966">Paper</a>
</p>
<br><br>

**Qwen-VL** æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLarge Vision Language Model, LVLMï¼‰ã€‚Qwen-VL å¯ä»¥ä»¥å›¾åƒã€æ–‡æœ¬ã€æ£€æµ‹æ¡†ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥æ–‡æœ¬å’Œæ£€æµ‹æ¡†ä½œä¸ºè¾“å‡ºã€‚Qwen-VL ç³»åˆ—æ¨¡å‹çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š

- **å¼ºå¤§çš„æ€§èƒ½**ï¼šåœ¨å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡çš„æ ‡å‡†è‹±æ–‡æµ‹è¯„ä¸­ï¼ˆZero-shot Captioning/VQA/DocVQA/Groundingï¼‰ä¸Šï¼Œå‡å–å¾—åŒç­‰é€šç”¨æ¨¡å‹å¤§å°ä¸‹æœ€å¥½æ•ˆæœï¼›
- **å¤šè¯­è¨€å¯¹è¯æ¨¡å‹**ï¼šå¤©ç„¶æ”¯æŒè‹±æ–‡ã€ä¸­æ–‡ç­‰å¤šè¯­è¨€å¯¹è¯ï¼Œç«¯åˆ°ç«¯æ”¯æŒå›¾ç‰‡é‡Œä¸­è‹±åŒè¯­çš„é•¿æ–‡æœ¬è¯†åˆ«ï¼›
- **å¤šå›¾äº¤é”™å¯¹è¯**ï¼šæ”¯æŒå¤šå›¾è¾“å…¥å’Œæ¯”è¾ƒï¼ŒæŒ‡å®šå›¾ç‰‡é—®ç­”ï¼Œå¤šå›¾æ–‡å­¦åˆ›ä½œç­‰ï¼›
- **é¦–ä¸ªæ”¯æŒä¸­æ–‡å¼€æ”¾åŸŸå®šä½çš„é€šç”¨æ¨¡å‹**ï¼šé€šè¿‡ä¸­æ–‡å¼€æ”¾åŸŸè¯­è¨€è¡¨è¾¾è¿›è¡Œæ£€æµ‹æ¡†æ ‡æ³¨ï¼›
- **ç»†ç²’åº¦è¯†åˆ«å’Œç†è§£**ï¼šç›¸æ¯”äºç›®å‰å…¶å®ƒå¼€æºLVLMä½¿ç”¨çš„224åˆ†è¾¨ç‡ï¼ŒQwen-VLæ˜¯é¦–ä¸ªå¼€æºçš„448åˆ†è¾¨ç‡çš„LVLMæ¨¡å‹ã€‚æ›´é«˜åˆ†è¾¨ç‡å¯ä»¥æå‡ç»†ç²’åº¦çš„æ–‡å­—è¯†åˆ«ã€æ–‡æ¡£é—®ç­”å’Œæ£€æµ‹æ¡†æ ‡æ³¨ã€‚

<br>
<p align="center">
    <img src="assets/demo_vl.gif" width="400"/>
<p>
<br>

ç›®å‰ï¼Œæˆ‘ä»¬æä¾›äº† Qwen-VL ç³»åˆ—çš„ä¸¤ä¸ªæ¨¡å‹ï¼š

- Qwen-VL: Qwen-VL ä»¥ Qwen-7B çš„é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºè¯­è¨€æ¨¡å‹çš„åˆå§‹åŒ–ï¼Œå¹¶ä»¥ [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) ä½œä¸ºè§†è§‰ç¼–ç å™¨çš„åˆå§‹åŒ–ï¼Œä¸­é—´åŠ å…¥å•å±‚éšæœºåˆå§‹åŒ–çš„ cross-attentionï¼Œç»è¿‡çº¦1.5Bçš„å›¾æ–‡æ•°æ®è®­ç»ƒå¾—åˆ°ã€‚æœ€ç»ˆå›¾åƒè¾“å…¥åˆ†è¾¨ç‡ä¸º448ã€‚
- Qwen-VL-Chat: åœ¨ Qwen-VL çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰AIåŠ©æ‰‹Qwen-VL-Chatï¼Œå®ƒæ”¯æŒæ›´çµæ´»çš„äº¤äº’æ–¹å¼ï¼ŒåŒ…æ‹¬å¤šå›¾ã€å¤šè½®é—®ç­”ã€åˆ›ä½œç­‰èƒ½åŠ›ã€‚
  <br>

## è¯„æµ‹

æˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦è¯„æµ‹äº†ä¸¤ä¸ªæ¨¡å‹çš„èƒ½åŠ›ï¼š

1. åœ¨**è‹±æ–‡æ ‡å‡† Benchmark** ä¸Šè¯„æµ‹æ¨¡å‹çš„åŸºç¡€ä»»åŠ¡èƒ½åŠ›ã€‚ç›®å‰è¯„æµ‹äº†å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡ï¼š
   
   - Zero-shot Captioning: è¯„æµ‹æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å›¾ç‰‡æè¿°èƒ½åŠ›ï¼›
   - General VQA: è¯„æµ‹æ¨¡å‹çš„é€šç”¨é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚åˆ¤æ–­é¢˜ã€é¢œè‰²ã€ä¸ªæ•°ã€ç±»ç›®ç­‰é—®ç­”èƒ½åŠ›ï¼›
   - Text-based VQAï¼šè¯„æµ‹æ¨¡å‹å¯¹äºå›¾ç‰‡ä¸­æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚æ–‡æ¡£é—®ç­”ã€å›¾è¡¨é—®ç­”ã€æ–‡å­—é—®ç­”ç­‰ï¼›
   - Referring Expression Compressionï¼šè¯„æµ‹æ¨¡å‹ç»™å®šç‰©ä½“æè¿°ç”»æ£€æµ‹æ¡†çš„èƒ½åŠ›ï¼›
2. **è¯•é‡‘çŸ³ (TouchStone)**ï¼šä¸ºäº†è¯„æµ‹æ¨¡å‹æ•´ä½“çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³ã€‚æˆ‘ä»¬ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„ Benchmarkï¼šTouchStoneã€‚åœ¨ TouchStone-v0.1 ä¸­ï¼š
   
   - è¯„æµ‹åŸºå‡†æ€»è®¡æ¶µç›– 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ã€‚åŒ…æ‹¬åŸºç¡€å±æ€§é—®ç­”ã€äººç‰©åœ°æ ‡é—®ç­”ã€å½±è§†ä½œå“é—®ç­”ã€è§†è§‰æ¨ç†ã€åäº‹å®æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œï¼Œå•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚
   - ä¸ºäº†å¼¥è¡¥ç›®å‰ GPT4 æ— æ³•ç›´æ¥è¯»å–å›¾ç‰‡çš„ç¼ºé™·ï¼Œæˆ‘ä»¬ç»™æ‰€æœ‰çš„å¸¦è¯„æµ‹å›¾ç‰‡æä¾›äº†**äººå·¥æ ‡æ³¨çš„å……åˆ†è¯¦ç»†æè¿°**ï¼Œå¹¶ä¸”å°†å›¾ç‰‡çš„è¯¦ç»†æè¿°ã€é—®é¢˜å’Œæ¨¡å‹çš„è¾“å‡ºç»“æœä¸€èµ·äº¤ç»™ GPT4 æ‰“åˆ†ã€‚
   - è¯„æµ‹åŒæ—¶åŒ…å«è‹±æ–‡ç‰ˆæœ¬å’Œä¸­æ–‡ç‰ˆæœ¬ã€‚

è¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š

Qwen-VLåœ¨å¤šä¸ªVLä»»åŠ¡ä¸Šç›¸æ¯”ç›®å‰SOTAçš„Generalist Modelséƒ½æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨èƒ½åŠ›èŒƒå›´ä¹Ÿè¦†ç›–æ›´åŠ å…¨é¢ã€‚

<p align="center">
    <img src="assets/radar.png" width="600"/>
<p>

### é›¶æ ·æœ¬å›¾åƒæè¿°ç”Ÿæˆï¼ˆZero-shot Image Captionï¼‰ åŠ é€šç”¨è§†è§‰é—®ç­”ï¼ˆGeneral VQAï¼‰

<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="2">Zero-shot Captioning</th>
    <th colspan="5">General VQA</th>
  </tr>
  <tr>
    <th>NoCaps</th>
    <th>Flickr30K</th>
    <th>VQAv2<sup>dev</sup></th>
    <th>OK-VQA</th>
    <th>GQA</th>
    <th>SciQA-Img<br>(0-shot)</th>
    <th>VizWiz<br>(0-shot)</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="10">Generalist<br>Models</td>
    <td>Flamingo-9B</td>
    <td>-</td>
    <td>61.5</td>
    <td>51.8</td>
    <td>44.7</td>
    <td>-</td>
    <td>-</td>
    <td>28.8</td>
  </tr>
  <tr>
    <td>Flamingo-80B</td>
    <td>-</td>
    <td>67.2</td>
    <td>56.3</td>
    <td>50.6</td>
    <td>-</td>
    <td>-</td>
    <td>31.6</td>
  </tr>
  <tr>
    <td>Unified-IO-XL</td>
    <td>100.0</td>
    <td>-</td>
    <td>77.9</td>
    <td>54.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Kosmos-1</td>
    <td>-</td>
    <td>67.1</td>
    <td>51.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>29.2</td>
  </tr>
  <tr>
    <td>Kosmos-2</td>
    <td>-</td>
    <td>66.7</td>
    <td>45.6</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>103.9</td>
    <td>71.6</td>
    <td>65.0</td>
    <td>45.9</td>
    <td>32.3</td>
    <td>61.0</td>
    <td>19.6</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td><strong>121.9</strong></td>
    <td>82.8</td>
    <td>-</td>
    <td>-</td>
    <td>49.5</td>
    <td>63.1</td>
    <td>33.4</td>
  </tr>
  <tr>
    <td>Shikra (Vicuna-13B)</td>
    <td>-</td>
    <td>73.9</td>
    <td>77.36</td>
    <td>47.16</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td><strong>Qwen-VL (Qwen-7B)</strong></td>
    <td>121.4</td>
    <td><b>85.8</b></td>
    <td><b>78.8</b></td>
    <td><b>58.6</b></td>
    <td><b>59.3</b></td>
    <td>67.1</td>
    <td>35.2</td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>63.6</td>
    <td>-</td>
    <td>-</td>
    <td>39.1</td>
  </tr> -->
  <tr>
    <td>Qwen-VL-Chat</td>
    <td>120.2</td>
    <td>81.0</td>
    <td>78.2</td>
    <td>56.6</td>
    <td>57.5</td>
    <td><b>68.2</b></td>
    <td><b>38.9</b></td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL-Chat (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>60.6</td>
    <td>-</td>
    <td>-</td>
    <td>44.45</td>
  </tr> -->
  <tr>
    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>
    <td>-</td>
    <td>127.0<br>(PALI-17B)</td>
    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>
    <td>86.1<br>(PALI-X<br>-55B)</td>
    <td>66.1<br>(PALI-X<br>-55B)</td>
    <td>72.1<br>(CFR)</td>
    <td>92.53<br>(LLaVa+<br>GPT-4)</td>
    <td>70.9<br>(PALI-X<br>-55B)</td>
  </tr>
</tbody>
</table>

- åœ¨ Zero-shot Captioning ä¸­ï¼ŒQwen-VL åœ¨ Flickr30K æ•°æ®é›†ä¸Šå–å¾—äº† **SOTA** çš„ç»“æœï¼Œå¹¶åœ¨ Nocaps æ•°æ®é›†ä¸Šå–å¾—äº†å’Œ InstructBlip å¯ç«äº‰çš„ç»“æœã€‚
- åœ¨ General VQA ä¸­ï¼ŒQwen-VL å–å¾—äº† LVLM æ¨¡å‹åŒç­‰é‡çº§å’Œè®¾å®šä¸‹ **SOTA** çš„ç»“æœã€‚

### æ–‡æœ¬å¯¼å‘çš„è§†è§‰é—®ç­”ï¼ˆText-oriented VQAï¼‰

<table>
<thead>
  <tr>
    <th>Model type</th>
    <th>Model</th>
    <th>TextVQA</th>
    <th>DocVQA</th>
    <th>ChartQA</th>
    <th>AI2D</th>
    <th>OCR-VQA</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="5">Generalist Models</td>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>42.4</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td>50.7</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>mPLUG-DocOwl (LLaMA-7B)</td>
    <td>52.6</td>
    <td>62.2</td>
    <td>57.4</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Pix2Struct-Large (1.3B)</td>
    <td>-</td>
    <td><b>76.6</b></td>
    <td>58.6</td>
    <td>42.1</td>
    <td>71.3</td>
  </tr>
  <tr>
    <td>Qwen-VL (Qwen-7B)</td>
    <td><b>63.8</b></td>
    <td>65.1</td>
    <td><b>65.7</b></td>
    <td><b>62.3</b></td>
    <td><b>75.7</b></td>
  </tr>
  <tr>
    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>
    <td>71.44</td>
    <td>80.0</td>
    <td>70.0</td>
    <td>81.2</td>
    <td>75.0</td>
  </tr>
</tbody>
</table>

- åœ¨æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”è¯„æµ‹ä¸Šï¼Œå–å¾—äº†å½“å‰è§„æ¨¡ä¸‹é€šç”¨ LVLM è¾¾åˆ°çš„æœ€å¥½ç»“æœã€‚
- åˆ†è¾¨ç‡å¯¹ä¸Šè¿°æŸå‡ ä¸ªè¯„æµ‹éå¸¸é‡è¦ï¼Œå¤§éƒ¨åˆ† 224 åˆ†è¾¨ç‡çš„å¼€æº LVLM æ¨¡å‹æ— æ³•å®Œæˆä»¥ä¸Šè¯„æµ‹ï¼Œæˆ–åªèƒ½é€šè¿‡åˆ‡å›¾çš„æ–¹å¼è§£å†³ã€‚Qwen-VL å°†åˆ†è¾¨ç‡æå‡åˆ° 448ï¼Œå¯ä»¥ç›´æ¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œä»¥ä¸Šè¯„æµ‹ã€‚Qwen-VL åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šç”šè‡³è¶…è¿‡äº† 1024 åˆ†è¾¨ç‡çš„ Pix2Struct-Large æ¨¡å‹ã€‚

### ç»†ç²’åº¦è§†è§‰å®šä½ï¼ˆReferring Expression Comprehensionï¼‰

<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="3">RefCOCO</th>
    <th colspan="3">RefCOCO+</th>
    <th colspan="2">RefCOCOg</th>
    <th>GRIT</th>
  </tr>
  <tr>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val-u</th>
    <th>test-u</th>
    <th>refexp</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="8">Generalist Models</td>
    <td>GPV-2</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>51.50</td>
  </tr>
  <tr>
    <td>OFA-L*</td>
    <td>79.96</td>
    <td>83.67</td>
    <td>76.39</td>
    <td>68.29</td>
    <td>76.00</td>
    <td>61.75</td>
    <td>67.57</td>
    <td>67.58</td>
    <td>61.70</td>
  </tr>
  <tr>
    <td>Unified-IO</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td><b>78.61</b></td>
  </tr>
  <tr>
    <td>VisionLLM-H</td>
    <td></td>
    <td>86.70</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Shikra-7B</td>
    <td>87.01</td>
    <td>90.61</td>
    <td>80.24 </td>
    <td>81.60</td>
    <td>87.36</td>
    <td>72.12</td>
    <td>82.27</td>
    <td>82.19</td>
    <td>69.34</td>
  </tr>
  <tr>
    <td>Shikra-13B</td>
    <td>87.83 </td>
    <td>91.11</td>
    <td>81.81</td>
    <td>82.89</td>
    <td>87.79</td>
    <td>74.41</td>
    <td>82.64</td>
    <td>83.16</td>
    <td>69.03</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B</td>
    <td><b>89.36</b></td>
    <td>92.26</td>
    <td><b>85.34</b></td>
    <td><b>83.12</b></td>
    <td>88.25</td>
    <td><b>77.21</b></td>
    <td>85.58</td>
    <td>85.48</td>
    <td>78.22</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B-Chat</td>
    <td>88.55</td>
    <td><b>92.27</b></td>
    <td>84.51</td>
    <td>82.82</td>
    <td><b>88.59</b></td>
    <td>76.79</td>
    <td><b>85.96</b></td>
    <td><b>86.32</b></td>
    <td>-</td>
  </tr>
  <tr>
    <td rowspan="3">Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>G-DINO-L</td>
    <td>90.56&nbsp;&nbsp;</td>
    <td>93.19</td>
    <td>88.24</td>
    <td>82.75</td>
    <td>88.95</td>
    <td>75.92</td>
    <td>86.13</td>
    <td>87.02</td>
    <td>-</td>
  </tr>
  <tr>
    <td>UNINEXT-H</td>
    <td>92.64 </td>
    <td>94.33</td>
    <td>91.46</td>
    <td>85.24</td>
    <td>89.63</td>
    <td>79.79</td>
    <td>88.73</td>
    <td>89.37</td>
    <td>-</td>
  </tr>
  <tr>
    <td>ONE-PEACE</td>
    <td>92.58 </td>
    <td>94.18</td>
    <td>89.26</td>
    <td>88.77</td>
    <td>92.21</td>
    <td>83.23</td>
    <td>89.22</td>
    <td>89.27</td>
    <td>-</td>
  </tr>
</tbody>
</table>

- åœ¨å®šä½ä»»åŠ¡ä¸Šï¼ŒQwen-VL å…¨é¢è¶…è¿‡ Shikra-13Bï¼Œå–å¾—äº†ç›®å‰ Generalist LVLM æ¨¡å‹ä¸Šåœ¨ Refcoco ä¸Šçš„ **SOTA**ã€‚
- Qwen-VL å¹¶æ²¡æœ‰åœ¨ä»»ä½•ä¸­æ–‡å®šä½æ•°æ®ä¸Šè®­ç»ƒè¿‡ï¼Œä½†é€šè¿‡ä¸­æ–‡ Caption æ•°æ®å’Œ è‹±æ–‡ Grounding æ•°æ®çš„è®­ç»ƒï¼Œå¯ä»¥ Zero-shot æ³›åŒ–å‡ºä¸­æ–‡ Grounding èƒ½åŠ›ã€‚

æˆ‘ä»¬æä¾›äº†ä»¥ä¸Š**æ‰€æœ‰**è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚è¯·é˜…è¯» [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) äº†è§£æ›´å¤šä¿¡æ¯ã€‚

### é—²èŠèƒ½åŠ›æµ‹è¯„

TouchStone æ˜¯ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³çš„åŸºå‡†ã€‚å®ƒæ¶µç›–äº† 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬åŸºç¡€å±æ€§ã€äººç‰©åœ°æ ‡ã€è§†è§‰æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œã€å•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚å…³äº TouchStone çš„è¯¦ç»†ä»‹ç»ï¼Œè¯·å‚è€ƒ[touchstone/README_CN.md](touchstone/README_CN.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

#### è‹±è¯­

| Model           | Score |
| --------------- | ----- |
| PandaGPT        | 488.5 |
| MiniGPT4        | 531.7 |
| InstructBLIP    | 552.4 |
| LLaMA-AdapterV2 | 590.1 |
| LLaVA           | 602.7 |
| mPLUG-Owl       | 605.4 |
| Qwen-VL-Chat    | 645.2 |

#### ä¸­æ–‡

| Model        | Score |
| ------------ | ----- |
| VisualGLM    | 247.1 |
| Qwen-VL-Chat | 401.2 |

Qwen-VL-Chat æ¨¡å‹åœ¨ä¸­è‹±æ–‡çš„å¯¹é½è¯„æµ‹ä¸­å‡å–å¾—å½“å‰ LVLM æ¨¡å‹ä¸‹çš„æœ€å¥½ç»“æœã€‚
<br>

## éƒ¨ç½²è¦æ±‚

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ ğŸ¤– ModelScope å’Œ ğŸ¤— Transformers å¿«é€Ÿä½¿ç”¨ Qwen-VL å’Œ Qwen-VL-Chatã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

```bash
pip install -r requirements.txt
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚å…³äºè§†è§‰æ¨¡å—çš„æ›´å¤šç”¨æ³•ï¼Œè¯·å‚è€ƒ[æ•™ç¨‹](TUTORIAL_zh.md)ã€‚

#### ğŸ¤— Transformers

å¦‚å¸Œæœ›ä½¿ç”¨ Qwen-VL-chat è¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ã€‚**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# è¯·æ³¨æ„ï¼šåˆ†è¯å™¨é»˜è®¤è¡Œä¸ºå·²æ›´æ”¹ä¸ºé»˜è®¤å…³é—­ç‰¹æ®Štokenæ”»å‡»é˜²æŠ¤ã€‚
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url
    {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒä»¬å¤„äºæ²™æ»©ä¸Šã€‚

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, 'æ¡†å‡ºå›¾ä¸­å‡»æŒçš„ä½ç½®', history=history)
print(response)
# <ref>å‡»æŒ</ref><box>(536,509),(588,602)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('1.jpg')
else:
  print("no box")
```

<p align="center">
    <img src="assets/demo_highfive.jpg" width="500"/>
<p>

è¿è¡ŒQwen-VLåŒæ ·éå¸¸ç®€å•ã€‚

<summary>è¿è¡ŒQwen-VL</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="cuda", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL", trust_remote_code=True)

query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url
    {'text': 'Generate the caption in English with grounding:'},
])
inputs = tokenizer(query, return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)
print(response)
# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>
image = tokenizer.draw_bbox_on_latest_picture(response)
if image:
  image.save('2.jpg')
else:
  print("no box")
```

<p align="center">
    <img src="assets/demo_spotting_caption.jpg" width="500"/>
<p>

#### ğŸ¤– ModelScope

é­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from modelscope import (
    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig
)
import torch
model_id = 'qwen/Qwen-VL-Chat'
revision = 'v1.0.0'

model_dir = snapshot_download(model_id, revision=revision)
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
if not hasattr(tokenizer, 'model_dir'):
    tokenizer.model_dir = model_dir
# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True).eval()

# æŒ‡å®šç”Ÿæˆè¶…å‚æ•°ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰
# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
# Either a local path or an url between <img></img> tags.
image_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'
response, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)
print(response)
# å›¾ä¸­æ˜¯ä¸€åå¹´è½»å¥³å­åœ¨æ²™æ»©ä¸Šå’Œå¥¹çš„ç‹—ç©è€ï¼Œç‹—çš„å“ç§æ˜¯æ‹‰å¸ƒæ‹‰å¤šã€‚å¥¹ä»¬ååœ¨æ²™æ»©ä¸Šï¼Œç‹—çš„å‰è…¿æŠ¬èµ·æ¥ï¼Œä¸äººäº’åŠ¨ã€‚

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, 'è¾“å‡ºå‡»æŒçš„æ£€æµ‹æ¡†', history=history)
print(response)
# <ref>"å‡»æŒ"</ref><box>(211,412),(577,891)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('output_chat.jpg')
else:
  print("no box")
```

<br>

## é‡åŒ–

### ç”¨æ³•

å½“å‰æˆ‘ä»¬æä¾›äº†åŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†Qwen-VL-Chatçš„Int4é‡åŒ–ç‰ˆæœ¬Qwen-VL-Chat-Int4 [ç‚¹å‡»æ­¤å¤„](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)ã€‚è¯¥æ¨¡å‹åœ¨æ•ˆæœè¯„æµ‹ä¸Šå‡ ä¹æ— æŸï¼Œå¹¶åœ¨æ˜¾å­˜å ç”¨å’Œæ¨ç†é€Ÿåº¦ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

ä¸‹æ–‡è¯´æ˜å¦‚ä½•ä½¿ç”¨è¯¥é‡åŒ–æ¨¡å‹ã€‚å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch2.0åŠä»¥ä¸Šã€transformers 4.32.0åŠä»¥ä¸Šï¼Œç­‰ï¼‰å¹¶å®‰è£…æ‰€éœ€çš„ä»£ç åº“ï¼š

```bash
pip install optimum
git clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ
pip install -v .
```

å¦‚é‡åˆ°å®‰è£… `auto-gptq` çš„é—®é¢˜ï¼Œå»ºè®®æ‚¨å‰å¾€å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ) å¯»æ‰¾åˆé€‚çš„wheelã€‚

éšåä½ ä¾¿å¯ä»¥æŒ‰ç…§ä¸Šè¿°ç”¨æ³•****ï¼Œè½»æ¾è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š

```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
# Either a local path or an url between <img></img> tags.
image_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'
response, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)
print(response)
```

### æ•ˆæœè¯„æµ‹

æˆ‘ä»¬åˆ—å‡ºä¸åŒç²¾åº¦ä¸‹æ¨¡å‹åœ¨è¯„æµ‹åŸºå‡† **[TouchStone](https://github.com/OFA-Sys/TouchStone)** ä¸Šçš„è¡¨ç°ï¼Œå¹¶å‘ç°é‡åŒ–æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾è‘—æ€§èƒ½æŸå¤±ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization | ZH         | EN            |
| ------------ | :--------: | :-----------: | 
| BF16         | 401.2      |    645.2      |
| Int4         | 386.6      |    651.4      |

### æ¨ç†é€Ÿåº¦

æˆ‘ä»¬æµ‹ç®—äº†åœ¨è¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼ˆå³258ä¸ªtokenï¼‰çš„æ¡ä»¶ä¸‹BF16å’ŒInt4çš„æ¨¡å‹ç”Ÿæˆ1792 (2048-258) å’Œ 7934 (8192-258) ä¸ªtokençš„å¹³å‡é€Ÿåº¦ã€‚

| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |
| ------------ | :-----------------: | :-----------------: |
| BF16         |        28.87        |        24.32        |
| Int4         |        37.79        |        34.34        |

æ¨ç†é€Ÿåº¦æµ‹ç®—æ˜¯åœ¨å•å¡ A100-SXM4-80G GPUä¸Šè¿è¡Œï¼Œä½¿ç”¨PyTorch 2.0.1åŠCUDA 11.4ã€‚

### GPUæ˜¾å­˜å ç”¨

æˆ‘ä»¬è¿˜æµ‹ç®—äº†åœ¨ä¸€å¼ å›¾ç‰‡è¾“å…¥çš„æ¡ä»¶ä¸‹BF16å’ŒInt4æ¨¡å‹ç”Ÿæˆ1792 (2048-258) å’Œ 7934 (8192-258) ä¸ªtokenæ‰€éœ€æ˜¾å­˜ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |
| ------------ | :---------------------------------: | :-----------------------------------: |
| BF16         |               22.60GB               |                28.01GB                |
| Int4         |               11.82GB               |                17.23GB                |

ä¸Šè¿°é€Ÿåº¦å’Œæ˜¾å­˜æµ‹ç®—ä½¿ç”¨[æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)å®Œæˆã€‚
<br>

## Demo

### Web UI

æˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š

```
pip install -r requirements_web_demo.txt
```

éšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š

```
python web_demo_mm.py
```

<br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜… [FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚
<br>

## ä½¿ç”¨åè®®

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwen-VLå’ŒQwen-VL-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚
<br>

## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„è®ºæ–‡å’Œä»£ç å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘:star: å’Œå¼•ç”¨ :pencil: :)

```BibTeX
@article{Qwen-VL,
  title={Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
```
<br>

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

