<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a> ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a>&nbsp
</p>
<br><br>

<p align="center">
    <img src="assets/logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-VL <a href="https://modelscope.cn/models/qwen/Qwen-VL/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-VL">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat <a href="https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-VL-Chat">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat-Int4 <a href="https://huggingface.co/Qwen/Qwen-VL-Chat-Int4">ğŸ¤—</a>
<br>
<a href="assets/wechat.png">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://arxiv.org/abs/2308.12966">Paper</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://github.com/camenduru/Qwen-VL-Chat-colab">Colab</a>&nbsp&nbsp | &nbsp <a href="TUTORIAL_zh.md">Tutorial</a>
</p>
<br><br>

## Qwen-VL-Plus

Qwen-VL-Plus æ˜¯ Qwen-VL çš„å‡çº§ç‰ˆï¼Œç›®å‰æ”¯æŒé€šè¿‡[ç½‘é¡µ](https://qianwen.aliyun.com), [ğŸ¤–](https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary)å’Œ[API](https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start)å…è´¹è®¿é—®ã€‚Qwen-VL-Plusçš„æ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ï¼š

- æ˜¾è‘—æé«˜å¤„ç†å›¾åƒä¸­çš„æ–‡æœ¬çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒä¸­ä¸­æ–‡çš„è¯†åˆ«èƒ½åŠ›ä¼˜äºGPT-4Vï¼šä½¿å…¶æˆä¸ºä»å›¾åƒä¸­æå–ã€ç»„ç»‡å’Œæ€»ç»“æ–‡æœ¬ä¿¡æ¯ç­‰ä»»åŠ¡çš„æœ‰ç”¨å·¥å…·ã€‚
- æ”¯æŒçš„å›¾åƒåˆ†è¾¨ç‡èŒƒå›´æ‰©å¤§ï¼šå…è®¸æ¨¡å‹å¤„ç†ä¸åŒå®½é«˜æ¯”å’Œå’Œæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼ŒåŒ…æ‹¬æ›´å¤§å’Œæ›´é•¿çš„å›¾åƒã€‚
- å¢å¼ºè§†è§‰æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼šç”¨æˆ·å¯ä»¥æ‹æ‘„ä¸€ä¸ªæ•°å­¦é—®é¢˜çš„ç…§ç‰‡å¹¶å‘é€ç»™Qwen-VL-Plusï¼Œå®ƒå°†å¸®åŠ©ç”¨æˆ·é€æ­¥è§£å†³å®ƒã€‚

<table>
<thead>
  <tr>
    <th>Model</th>
    <th>DocVQA<sup>(test)</sup></th>
    <th>ChartQA<sup>(test)</sup></th>
    <th>AI2D<sup>(test)</sup></th>
    <th>TextVQA<sup>(val)</sup></th>
    <th>MMMU<sup>(val)</sup></th>
    <th>MathVista<sup>(testmini)</sup></th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td>Other Best Generalist LVLM</td>
    <td>81.6%<br><sub>(CogAgent)</sub></td>
    <td>68.4%<br><sub>(CogAgent)</sub></td>
    <td>73.7%<br><sub>(Fuyu-Medium)</sub></td>
    <td>76.1%<br><sub>(CogAgent)</sub></td>
    <td>36.4%<br><sub>(LLaVA-1.5)</sub></td>
    <td>36.7%<br><sub>(SPHINX-V2)</sub></td>
  </tr>
  <tr>
    <td>Gemini Pro</td>
    <td>88.1%</td>
    <td>74.1%</td>
    <td>73.9%</td>
    <td>74.6%</td>
    <td>47.9%</td>
    <td>45.2%</td>
  </tr>
  <tr>
    <td>Gemini Ultra</td>
    <td>90.9%</td>
    <td><b>80.8%</b></td>
    <td><b>79.5%</b></td>
    <td><b>82.3%</b></td>
    <td><b>59.4%</b></td>
    <td><b>53.0%</b></td>
  </tr>
  <tr>
    <td>GPT-4V</td>
    <td>88.4%</td>
    <td>78.5%</td>
    <td>78.2%</td>
    <td>78.0%</td>
    <td>56.8%</td>
    <td>49.9%</td>
  </tr>
  <tr>
    <td><b>Qwen-VL-Plus</b></td>
    <td><b>91.4% <sup>1</sup></b></td>
    <td>78.1% <sup>3</sup></td>
    <td>75.9% <sup>3</sup></td>
    <td>78.9% <sup>2</sup></td>
    <td>46.5% <sup>4</sup></td>
    <td>41.0% <sup>4</sup></td>
  </tr>
</tbody>
</table>

æ‰€æœ‰è¯„æµ‹éƒ½æ˜¯åœ¨ä¸ä½¿ç”¨ä»»ä½•å¤–éƒ¨OCRå·¥å…·(â€œonly pixelâ€)çš„æƒ…å†µä¸‹è·å¾—çš„ã€‚æ­¤å¤–ï¼ŒQwen-VL-Plusä¹Ÿä»¥ä¸‹æ¦œå•ä¸­å®ç°äº†SOTAï¼š[MM-Bench](https://mmbench.opencompass.org.cn/leaderboard), [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), [Seed-Bench-v1](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard).

## Qwen-VL

**Qwen-VL** æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLarge Vision Language Model, LVLMï¼‰ã€‚Qwen-VL å¯ä»¥ä»¥å›¾åƒã€æ–‡æœ¬ã€æ£€æµ‹æ¡†ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥æ–‡æœ¬å’Œæ£€æµ‹æ¡†ä½œä¸ºè¾“å‡ºã€‚Qwen-VL ç³»åˆ—æ¨¡å‹çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š

- **å¼ºå¤§çš„æ€§èƒ½**ï¼šåœ¨å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡çš„æ ‡å‡†è‹±æ–‡æµ‹è¯„ä¸­ï¼ˆZero-shot Captioning/VQA/DocVQA/Groundingï¼‰ä¸Šï¼Œå‡å–å¾—åŒç­‰é€šç”¨æ¨¡å‹å¤§å°ä¸‹æœ€å¥½æ•ˆæœï¼›
- **å¤šè¯­è¨€å¯¹è¯æ¨¡å‹**ï¼šå¤©ç„¶æ”¯æŒè‹±æ–‡ã€ä¸­æ–‡ç­‰å¤šè¯­è¨€å¯¹è¯ï¼Œç«¯åˆ°ç«¯æ”¯æŒå›¾ç‰‡é‡Œä¸­è‹±åŒè¯­çš„é•¿æ–‡æœ¬è¯†åˆ«ï¼›
- **å¤šå›¾äº¤é”™å¯¹è¯**ï¼šæ”¯æŒå¤šå›¾è¾“å…¥å’Œæ¯”è¾ƒï¼ŒæŒ‡å®šå›¾ç‰‡é—®ç­”ï¼Œå¤šå›¾æ–‡å­¦åˆ›ä½œç­‰ï¼›
- **é¦–ä¸ªæ”¯æŒä¸­æ–‡å¼€æ”¾åŸŸå®šä½çš„é€šç”¨æ¨¡å‹**ï¼šé€šè¿‡ä¸­æ–‡å¼€æ”¾åŸŸè¯­è¨€è¡¨è¾¾è¿›è¡Œæ£€æµ‹æ¡†æ ‡æ³¨ï¼›
- **ç»†ç²’åº¦è¯†åˆ«å’Œç†è§£**ï¼šç›¸æ¯”äºç›®å‰å…¶å®ƒå¼€æºLVLMä½¿ç”¨çš„224åˆ†è¾¨ç‡ï¼ŒQwen-VLæ˜¯é¦–ä¸ªå¼€æºçš„448åˆ†è¾¨ç‡çš„LVLMæ¨¡å‹ã€‚æ›´é«˜åˆ†è¾¨ç‡å¯ä»¥æå‡ç»†ç²’åº¦çš„æ–‡å­—è¯†åˆ«ã€æ–‡æ¡£é—®ç­”å’Œæ£€æµ‹æ¡†æ ‡æ³¨ã€‚

<br>
<p align="center">
    <img src="assets/demo_vl.gif" width="400"/>
<p>
<br>

ç›®å‰ï¼Œæˆ‘ä»¬æä¾›äº† Qwen-VL ç³»åˆ—çš„ä¸¤ä¸ªæ¨¡å‹ï¼š

- Qwen-VL: Qwen-VL ä»¥ Qwen-7B çš„é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºè¯­è¨€æ¨¡å‹çš„åˆå§‹åŒ–ï¼Œå¹¶ä»¥ [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) ä½œä¸ºè§†è§‰ç¼–ç å™¨çš„åˆå§‹åŒ–ï¼Œä¸­é—´åŠ å…¥å•å±‚éšæœºåˆå§‹åŒ–çš„ cross-attentionï¼Œç»è¿‡çº¦1.5Bçš„å›¾æ–‡æ•°æ®è®­ç»ƒå¾—åˆ°ã€‚æœ€ç»ˆå›¾åƒè¾“å…¥åˆ†è¾¨ç‡ä¸º448ã€‚
- Qwen-VL-Chat: åœ¨ Qwen-VL çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰AIåŠ©æ‰‹Qwen-VL-Chatï¼Œå®ƒæ”¯æŒæ›´çµæ´»çš„äº¤äº’æ–¹å¼ï¼ŒåŒ…æ‹¬å¤šå›¾ã€å¤šè½®é—®ç­”ã€åˆ›ä½œç­‰èƒ½åŠ›ã€‚
  <br>

## æ–°é—»
* 2023å¹´11æœˆ28æ—¥ Qwen-VLå•æ¨¡å‹åœ¨[DOCVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1)è¾¾åˆ°äº†æœ€å¼ºæ°´å¹³ï¼Œè¶…è¶Šäº†GPT4V,PALI-Xï¼Œä¸æ­¤åŒæ—¶å®ƒè¿˜æ˜¯ä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œç›´æ¥è¾“å…¥å›¾ç‰‡å°±èƒ½å¸®ä½ åˆ†æç†è§£å„ç§ä»»åŠ¡ã€‚ç›´æ¥è®¿é—®[å¤šæ¨¡æ€tab](https://qianwen.aliyun.com)å°±èƒ½ä½“éªŒæ–°æ¨¡å‹ã€‚
* 2023å¹´9æœˆ12æ—¥ æ›´æ–°Qwen-VL-Chatæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æœ‰æ›´é²æ£’çš„ä¸­æ–‡æŒ‡ä»¤è·Ÿéšï¼Œæ›´å¥½çš„ç½‘é¡µå’Œè¡¨æ ¼å›¾ç‰‡ç†è§£å’Œé—®ç­”èƒ½åŠ›ä»¥åŠæ›´å¥½çš„å¯¹è¯è¡¨ç°(Touchstone: ä¸­æ–‡: 401.2->481.7, è‹±æ–‡: 645.2->711.6)ã€‚
* 2023å¹´9æœˆ12æ—¥ æ”¯æŒQwen-VLå’ŒQwen-VL-Chatçš„å¾®è°ƒï¼Œå…¶ä¸­åŒ…æ‹¬å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRA
* 2023å¹´9æœˆ8æ—¥ æ„Ÿè°¢[camenduru](https://github.com/camenduru)è´¡çŒ®äº†[Colab](https://github.com/camenduru/Qwen-VL-Chat-colab)ç¤ºä¾‹ï¼Œæ¯ä¸ªäººéƒ½å¯ä»¥ä»¥æ­¤ä¸ºæ•™ç¨‹ï¼Œåœ¨12Gçš„GPUä¸Šåšæœ¬åœ°æˆ–åœ¨çº¿çš„Demoã€‚
* 2023å¹´9æœˆ5æ—¥ åœ¨ç¤¾åŒºå¤šæ¨¡æ€é€šç”¨æ¨¡å‹æ¦œå• [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) ä¸Šå–å¾—äº†æ„ŸçŸ¥å’Œè®¤çŸ¥åŒèµ›é“çš„å½“å‰æœ€å¥½ç»“æœã€‚
* 2023å¹´9æœˆ4æ—¥ åœ¨ç¤¾åŒºå¤šæ¨¡æ€é€šç”¨æ¨¡å‹æ¦œå• [SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) ä¸Šå–å¾—äº†å›¾åƒç†è§£å’Œè§†é¢‘ç†è§£çš„å½“å‰æœ€å¥½ç»“æœã€‚
* 2023å¹´9æœˆ1æ—¥ å‘å¸ƒ[TouchStone](https://github.com/OFA-Sys/TouchStone) æµ‹è¯„, è¿™æ˜¯ä¸€ä¸ªç»¼åˆè¯„ä¼°LVLMèƒ½åŠ›çš„æµ‹è¯„,å®ƒä¸ä»…è€ƒå¯Ÿæ¨¡å‹çš„è§†è§‰æè¿°å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿˜åŒ…æ‹¬æ ¹æ®è§†è§‰å†…å®¹çš„æ–‡å­¦åˆ›ä½œèƒ½åŠ›ã€‚åŒæ—¶å®ƒæ˜¯å°†å¤šæ¨¡æ€ä¿¡æ¯ç”¨æ–‡æœ¬è¡¨è¿°å¹¶ç”¨LLMsè¿›è¡Œè¯„ä¼°çš„æ–¹æ³•ã€‚
* 2023å¹´8æœˆ31æ—¥ å‘å¸ƒQwen-VL-Chaté‡åŒ–æ¨¡å‹ï¼Œ**Qwen-VL-Chat-Int4**,è¯¥æ¨¡å‹æ˜¾å­˜å ç”¨ä½ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åŠç²¾åº¦æ¨¡å‹æ˜¾è‘—æå‡ï¼Œåœ¨åŸºå‡†è¯„æµ‹ä¸Šæ•ˆæœæŸå¤±è¾ƒå°ã€‚
* 2023å¹´8æœˆ22æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging FaceåŒæ­¥æ¨å‡ºQwen-VLå’ŒQwen-VL-Chatæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ª[è®ºæ–‡](https://arxiv.org/abs/2308.12966)ä»‹ç»äº†ç›¸å…³çš„æ¨¡å‹ç»“æ„ã€è®­ç»ƒç»†èŠ‚å’Œæ¨¡å‹è¡¨ç°ã€‚


## è¯„æµ‹

æˆ‘ä»¬ä»ä¸‰ä¸ªè§’åº¦è¯„æµ‹äº†æ¨¡å‹çš„èƒ½åŠ›ï¼š

1. åœ¨**è‹±æ–‡æ ‡å‡† Benchmark** ä¸Šè¯„æµ‹æ¨¡å‹çš„åŸºç¡€ä»»åŠ¡èƒ½åŠ›ã€‚ç›®å‰è¯„æµ‹äº†å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡ï¼š
   
   - Zero-shot Captioning: è¯„æµ‹æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å›¾ç‰‡æè¿°èƒ½åŠ›ï¼›
   - General VQA: è¯„æµ‹æ¨¡å‹çš„é€šç”¨é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚åˆ¤æ–­é¢˜ã€é¢œè‰²ã€ä¸ªæ•°ã€ç±»ç›®ç­‰é—®ç­”èƒ½åŠ›ï¼›
   - Text-based VQAï¼šè¯„æµ‹æ¨¡å‹å¯¹äºå›¾ç‰‡ä¸­æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚æ–‡æ¡£é—®ç­”ã€å›¾è¡¨é—®ç­”ã€æ–‡å­—é—®ç­”ç­‰ï¼›
   - Referring Expression Compressionï¼šè¯„æµ‹æ¨¡å‹ç»™å®šç‰©ä½“æè¿°ç”»æ£€æµ‹æ¡†çš„èƒ½åŠ›ï¼›
2. **è¯•é‡‘çŸ³ (TouchStone)**ï¼šä¸ºäº†è¯„æµ‹æ¨¡å‹æ•´ä½“çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³ã€‚æˆ‘ä»¬ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„ Benchmarkï¼šTouchStoneã€‚åœ¨ TouchStone-v0.1 ä¸­ï¼š
   
   - è¯„æµ‹åŸºå‡†æ€»è®¡æ¶µç›– 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ã€‚åŒ…æ‹¬åŸºç¡€å±æ€§é—®ç­”ã€äººç‰©åœ°æ ‡é—®ç­”ã€å½±è§†ä½œå“é—®ç­”ã€è§†è§‰æ¨ç†ã€åäº‹å®æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œï¼Œå•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚
   - ä¸ºäº†å¼¥è¡¥ç›®å‰ GPT4 æ— æ³•ç›´æ¥è¯»å–å›¾ç‰‡çš„ç¼ºé™·ï¼Œæˆ‘ä»¬ç»™æ‰€æœ‰çš„å¸¦è¯„æµ‹å›¾ç‰‡æä¾›äº†**äººå·¥æ ‡æ³¨çš„å……åˆ†è¯¦ç»†æè¿°**ï¼Œå¹¶ä¸”å°†å›¾ç‰‡çš„è¯¦ç»†æè¿°ã€é—®é¢˜å’Œæ¨¡å‹çš„è¾“å‡ºç»“æœä¸€èµ·äº¤ç»™ GPT4 æ‰“åˆ†ã€‚
   - è¯„æµ‹åŒæ—¶åŒ…å«è‹±æ–‡ç‰ˆæœ¬å’Œä¸­æ–‡ç‰ˆæœ¬ã€‚

3. **å…¶å®ƒå¤šæ¨¡æ€é€šç”¨æ¨¡å‹æ¦œå•**ï¼šæˆ‘ä»¬ä¹Ÿåœ¨å…¶å®ƒå¤šæ¨¡æ€é€šç”¨æ¨¡å‹æ¦œå•ä¸­è¯„æµ‹äº†æ¨¡å‹çš„èƒ½åŠ›ï¼š
   
   - MME Benchmark: æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»¼åˆè¯„ä»·åŸºå‡†ã€‚å®ƒåœ¨æ€»å…±14ä¸ªå­ä»»åŠ¡ä¸Šè¯„æµ‹**æ„ŸçŸ¥å’Œè®¤çŸ¥**èƒ½åŠ›ï¼ŒQwen-VL-Chatåœ¨è¿™ä¸¤ä¸ªæ€»ç»´åº¦ä¸Šéƒ½å®ç°äº†å½“å‰æœ€å¥½ç»“æœã€‚
   - SEED-Bench: æ˜¯ä¸€ä¸ªåŒ…å«1.9ä¸‡é€‰æ‹©é¢˜çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯„ï¼Œé€šè¿‡äººå·¥æ³¨é‡Šçš„ç»“æœè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ¶µç›–12ä¸ªè¯„ä¼°ç»´åº¦ï¼ŒåŒ…æ‹¬**å›¾åƒå’Œè§†é¢‘ç†è§£**ï¼ŒQwen-VLå’ŒQwen-VL-chatåœ¨è¿™ä¸ªåŸºå‡†ä¸Šå®ç°äº†å½“å‰æœ€å¥½ç»“æœã€‚

è¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š

Qwen-VLåœ¨å¤šä¸ªVLä»»åŠ¡ä¸Šç›¸æ¯”ç›®å‰SOTAçš„Generalist Modelséƒ½æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨èƒ½åŠ›èŒƒå›´ä¹Ÿè¦†ç›–æ›´åŠ å…¨é¢ã€‚

<p align="center">
    <img src="assets/radar.png" width="600"/>
<p>

### é›¶æ ·æœ¬å›¾åƒæè¿°ç”Ÿæˆï¼ˆZero-shot Image Captionï¼‰ åŠ é€šç”¨è§†è§‰é—®ç­”ï¼ˆGeneral VQAï¼‰

<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="2">Zero-shot Captioning</th>
    <th colspan="5">General VQA</th>
  </tr>
  <tr>
    <th>NoCaps</th>
    <th>Flickr30K</th>
    <th>VQAv2<sup>dev</sup></th>
    <th>OK-VQA</th>
    <th>GQA</th>
    <th>SciQA-Img<br>(0-shot)</th>
    <th>VizWiz<br>(0-shot)</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="10">Generalist<br>Models</td>
    <td>Flamingo-9B</td>
    <td>-</td>
    <td>61.5</td>
    <td>51.8</td>
    <td>44.7</td>
    <td>-</td>
    <td>-</td>
    <td>28.8</td>
  </tr>
  <tr>
    <td>Flamingo-80B</td>
    <td>-</td>
    <td>67.2</td>
    <td>56.3</td>
    <td>50.6</td>
    <td>-</td>
    <td>-</td>
    <td>31.6</td>
  </tr>
  <tr>
    <td>Unified-IO-XL</td>
    <td>100.0</td>
    <td>-</td>
    <td>77.9</td>
    <td>54.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Kosmos-1</td>
    <td>-</td>
    <td>67.1</td>
    <td>51.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>29.2</td>
  </tr>
  <tr>
    <td>Kosmos-2</td>
    <td>-</td>
    <td>66.7</td>
    <td>45.6</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>103.9</td>
    <td>71.6</td>
    <td>65.0</td>
    <td>45.9</td>
    <td>32.3</td>
    <td>61.0</td>
    <td>19.6</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td><strong>121.9</strong></td>
    <td>82.8</td>
    <td>-</td>
    <td>-</td>
    <td>49.5</td>
    <td>63.1</td>
    <td>33.4</td>
  </tr>
  <tr>
    <td>Shikra (Vicuna-13B)</td>
    <td>-</td>
    <td>73.9</td>
    <td>77.36</td>
    <td>47.16</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td><strong>Qwen-VL (Qwen-7B)</strong></td>
    <td>121.4</td>
    <td><b>85.8</b></td>
    <td><b>78.8</b></td>
    <td><b>58.6</b></td>
    <td><b>59.3</b></td>
    <td>67.1</td>
    <td>35.2</td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>63.6</td>
    <td>-</td>
    <td>-</td>
    <td>39.1</td>
  </tr> -->
  <tr>
    <td>Qwen-VL-Chat</td>
    <td>120.2</td>
    <td>81.0</td>
    <td>78.2</td>
    <td>56.6</td>
    <td>57.5</td>
    <td><b>68.2</b></td>
    <td><b>38.9</b></td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL-Chat (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>60.6</td>
    <td>-</td>
    <td>-</td>
    <td>44.45</td>
  </tr> -->
  <tr>
    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>
    <td>-</td>
    <td>127.0<br>(PALI-17B)</td>
    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>
    <td>86.1<br>(PALI-X<br>-55B)</td>
    <td>66.1<br>(PALI-X<br>-55B)</td>
    <td>72.1<br>(CFR)</td>
    <td>92.53<br>(LLaVa+<br>GPT-4)</td>
    <td>70.9<br>(PALI-X<br>-55B)</td>
  </tr>
</tbody>
</table>

- åœ¨ Zero-shot Captioning ä¸­ï¼ŒQwen-VL åœ¨ Flickr30K æ•°æ®é›†ä¸Šå–å¾—äº† **SOTA** çš„ç»“æœï¼Œå¹¶åœ¨ Nocaps æ•°æ®é›†ä¸Šå–å¾—äº†å’Œ InstructBlip å¯ç«äº‰çš„ç»“æœã€‚
- åœ¨ General VQA ä¸­ï¼ŒQwen-VL å–å¾—äº† LVLM æ¨¡å‹åŒç­‰é‡çº§å’Œè®¾å®šä¸‹ **SOTA** çš„ç»“æœã€‚

### æ–‡æœ¬å¯¼å‘çš„è§†è§‰é—®ç­”ï¼ˆText-oriented VQAï¼‰

<table>
<thead>
  <tr>
    <th>Model type</th>
    <th>Model</th>
    <th>TextVQA</th>
    <th>DocVQA</th>
    <th>ChartQA</th>
    <th>AI2D</th>
    <th>OCR-VQA</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="5">Generalist Models</td>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>42.4</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td>50.7</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>mPLUG-DocOwl (LLaMA-7B)</td>
    <td>52.6</td>
    <td>62.2</td>
    <td>57.4</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Pix2Struct-Large (1.3B)</td>
    <td>-</td>
    <td><b>76.6</b></td>
    <td>58.6</td>
    <td>42.1</td>
    <td>71.3</td>
  </tr>
  <tr>
    <td>Qwen-VL (Qwen-7B)</td>
    <td><b>63.8</b></td>
    <td>65.1</td>
    <td><b>65.7</b></td>
    <td><b>62.3</b></td>
    <td><b>75.7</b></td>
  </tr>
  <tr>
    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>
    <td>71.44</td>
    <td>80.0</td>
    <td>70.0</td>
    <td>81.2</td>
    <td>75.0</td>
  </tr>
</tbody>
</table>

- åœ¨æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”è¯„æµ‹ä¸Šï¼Œå–å¾—äº†å½“å‰è§„æ¨¡ä¸‹é€šç”¨ LVLM è¾¾åˆ°çš„æœ€å¥½ç»“æœã€‚
- åˆ†è¾¨ç‡å¯¹ä¸Šè¿°æŸå‡ ä¸ªè¯„æµ‹éå¸¸é‡è¦ï¼Œå¤§éƒ¨åˆ† 224 åˆ†è¾¨ç‡çš„å¼€æº LVLM æ¨¡å‹æ— æ³•å®Œæˆä»¥ä¸Šè¯„æµ‹ï¼Œæˆ–åªèƒ½é€šè¿‡åˆ‡å›¾çš„æ–¹å¼è§£å†³ã€‚Qwen-VL å°†åˆ†è¾¨ç‡æå‡åˆ° 448ï¼Œå¯ä»¥ç›´æ¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œä»¥ä¸Šè¯„æµ‹ã€‚Qwen-VL åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šç”šè‡³è¶…è¿‡äº† 1024 åˆ†è¾¨ç‡çš„ Pix2Struct-Large æ¨¡å‹ã€‚

### ç»†ç²’åº¦è§†è§‰å®šä½ï¼ˆReferring Expression Comprehensionï¼‰

<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="3">RefCOCO</th>
    <th colspan="3">RefCOCO+</th>
    <th colspan="2">RefCOCOg</th>
    <th>GRIT</th>
  </tr>
  <tr>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val-u</th>
    <th>test-u</th>
    <th>refexp</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="8">Generalist Models</td>
    <td>GPV-2</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>51.50</td>
  </tr>
  <tr>
    <td>OFA-L*</td>
    <td>79.96</td>
    <td>83.67</td>
    <td>76.39</td>
    <td>68.29</td>
    <td>76.00</td>
    <td>61.75</td>
    <td>67.57</td>
    <td>67.58</td>
    <td>61.70</td>
  </tr>
  <tr>
    <td>Unified-IO</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td><b>78.61</b></td>
  </tr>
  <tr>
    <td>VisionLLM-H</td>
    <td></td>
    <td>86.70</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Shikra-7B</td>
    <td>87.01</td>
    <td>90.61</td>
    <td>80.24 </td>
    <td>81.60</td>
    <td>87.36</td>
    <td>72.12</td>
    <td>82.27</td>
    <td>82.19</td>
    <td>69.34</td>
  </tr>
  <tr>
    <td>Shikra-13B</td>
    <td>87.83 </td>
    <td>91.11</td>
    <td>81.81</td>
    <td>82.89</td>
    <td>87.79</td>
    <td>74.41</td>
    <td>82.64</td>
    <td>83.16</td>
    <td>69.03</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B</td>
    <td><b>89.36</b></td>
    <td>92.26</td>
    <td><b>85.34</b></td>
    <td><b>83.12</b></td>
    <td>88.25</td>
    <td><b>77.21</b></td>
    <td>85.58</td>
    <td>85.48</td>
    <td>78.22</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B-Chat</td>
    <td>88.55</td>
    <td><b>92.27</b></td>
    <td>84.51</td>
    <td>82.82</td>
    <td><b>88.59</b></td>
    <td>76.79</td>
    <td><b>85.96</b></td>
    <td><b>86.32</b></td>
    <td>-</td>
  </tr>
  <tr>
    <td rowspan="3">Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>G-DINO-L</td>
    <td>90.56&nbsp;&nbsp;</td>
    <td>93.19</td>
    <td>88.24</td>
    <td>82.75</td>
    <td>88.95</td>
    <td>75.92</td>
    <td>86.13</td>
    <td>87.02</td>
    <td>-</td>
  </tr>
  <tr>
    <td>UNINEXT-H</td>
    <td>92.64 </td>
    <td>94.33</td>
    <td>91.46</td>
    <td>85.24</td>
    <td>89.63</td>
    <td>79.79</td>
    <td>88.73</td>
    <td>89.37</td>
    <td>-</td>
  </tr>
  <tr>
    <td>ONE-PEACE</td>
    <td>92.58 </td>
    <td>94.18</td>
    <td>89.26</td>
    <td>88.77</td>
    <td>92.21</td>
    <td>83.23</td>
    <td>89.22</td>
    <td>89.27</td>
    <td>-</td>
  </tr>
</tbody>
</table>

- åœ¨å®šä½ä»»åŠ¡ä¸Šï¼ŒQwen-VL å…¨é¢è¶…è¿‡ Shikra-13Bï¼Œå–å¾—äº†ç›®å‰ Generalist LVLM æ¨¡å‹ä¸Šåœ¨ Refcoco ä¸Šçš„ **SOTA**ã€‚
- Qwen-VL å¹¶æ²¡æœ‰åœ¨ä»»ä½•ä¸­æ–‡å®šä½æ•°æ®ä¸Šè®­ç»ƒè¿‡ï¼Œä½†é€šè¿‡ä¸­æ–‡ Caption æ•°æ®å’Œ è‹±æ–‡ Grounding æ•°æ®çš„è®­ç»ƒï¼Œå¯ä»¥ Zero-shot æ³›åŒ–å‡ºä¸­æ–‡ Grounding èƒ½åŠ›ã€‚

æˆ‘ä»¬æä¾›äº†ä»¥ä¸Š**æ‰€æœ‰**è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚è¯·é˜…è¯» [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) äº†è§£æ›´å¤šä¿¡æ¯ã€‚

### å¯¹è¯èƒ½åŠ›æµ‹è¯„

TouchStone æ˜¯ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³çš„åŸºå‡†ã€‚å®ƒæ¶µç›–äº† 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬åŸºç¡€å±æ€§ã€äººç‰©åœ°æ ‡ã€è§†è§‰æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œã€å•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚å…³äº TouchStone çš„è¯¦ç»†ä»‹ç»ï¼Œè¯·å‚è€ƒ[touchstone/README_CN.md](touchstone/README_CN.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

#### è‹±è¯­

| Model            | Score |
| ---------------- | ----- |
| PandaGPT         | 488.5 |
| MiniGPT4         | 531.7 |
| InstructBLIP     | 552.4 |
| LLaMA-AdapterV2  | 590.1 |
| LLaVA            | 602.7 |
| mPLUG-Owl        | 605.4 |
| Qwen-VL-Chat     | 645.2 |
| Qwen-VL-Chat-1.1 | 711.6 |

#### ä¸­æ–‡

| Model            | Score |
| ---------------- | ----- |
| VisualGLM        | 247.1 |
| Qwen-VL-Chat     | 401.2 |
| Qwen-VL-Chat-1.1 | 481.7 |

Qwen-VL-Chat æ¨¡å‹åœ¨ä¸­è‹±æ–‡çš„å¯¹é½è¯„æµ‹ä¸­å‡å–å¾—å½“å‰ LVLM æ¨¡å‹ä¸‹çš„æœ€å¥½ç»“æœã€‚
<br>

### å…¶å®ƒæ¦œå•æµ‹è¯„

#### MME Benchmark

MMEæ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»¼åˆè¯„ä»·åŸºå‡†ã€‚å®ƒåœ¨æ€»å…±14ä¸ªå­ä»»åŠ¡ä¸Šè¯„æµ‹**æ„ŸçŸ¥å’Œè®¤çŸ¥**èƒ½åŠ›ã€‚Qwen-VL-Chatåœ¨è¿™ä¸ªåŸºå‡†ä¸Šå®ç°äº†SOTAsã€‚å®Œæ•´å¤ç°[è§æ­¤](eval_mm/mme/EVAL_MME.md).

<p align="center">
    <img src="eval_mm/mme/perception.jpg" width="600"/>
<p>
<p align="center">
    <img src="eval_mm/mme/cognition.jpg" width="600"/>
<p>

#### SEED-Bench

SEED-Benchæ˜¯ä¸€ä¸ªåŒ…å«1.9ä¸‡é€‰æ‹©é¢˜çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯„ï¼Œé€šè¿‡äººå·¥æ³¨é‡Šçš„ç»“æœè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ¶µç›–12ä¸ªè¯„ä¼°ç»´åº¦ï¼ŒåŒ…æ‹¬**å›¾åƒå’Œè§†é¢‘ç†è§£**ã€‚Qwen-VLå’ŒQwen-VL-chatåœ¨è¿™ä¸ªåŸºå‡†ä¸Šå®ç°äº†SOTAsã€‚å®Œæ•´å¤ç°[è§æ­¤](eval_mm/seed_bench/EVAL_SEED.md)ã€‚

<p align="center">
    <img src="eval_mm/seed_bench/leaderboard.jpg"/>
<p>

## éƒ¨ç½²è¦æ±‚

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ ğŸ¤– ModelScope å’Œ ğŸ¤— Transformers å¿«é€Ÿä½¿ç”¨ Qwen-VL å’Œ Qwen-VL-Chatã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

```bash
pip install -r requirements.txt
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚å…³äºè§†è§‰æ¨¡å—çš„æ›´å¤šç”¨æ³•ï¼Œè¯·å‚è€ƒ[æ•™ç¨‹](TUTORIAL_zh.md)ã€‚

#### ğŸ¤— Transformers

å¦‚å¸Œæœ›ä½¿ç”¨ Qwen-VL-chat è¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ã€‚**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# è¯·æ³¨æ„ï¼šåˆ†è¯å™¨é»˜è®¤è¡Œä¸ºå·²æ›´æ”¹ä¸ºé»˜è®¤å…³é—­ç‰¹æ®Štokenæ”»å‡»é˜²æŠ¤ã€‚
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url
    {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒä»¬å¤„äºæ²™æ»©ä¸Šã€‚

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, 'æ¡†å‡ºå›¾ä¸­å‡»æŒçš„ä½ç½®', history=history)
print(response)
# <ref>å‡»æŒ</ref><box>(536,509),(588,602)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('1.jpg')
else:
  print("no box")
```

<p align="center">
    <img src="assets/demo_highfive.jpg" width="500"/>
<p>

è¿è¡ŒQwen-VLåŒæ ·éå¸¸ç®€å•ã€‚

<summary>è¿è¡ŒQwen-VL</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="cuda", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL", trust_remote_code=True)

query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url
    {'text': 'Generate the caption in English with grounding:'},
])
inputs = tokenizer(query, return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)
print(response)
# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>
image = tokenizer.draw_bbox_on_latest_picture(response)
if image:
  image.save('2.jpg')
else:
  print("no box")
```

<p align="center">
    <img src="assets/demo_spotting_caption.jpg" width="500"/>
<p>

è‹¥åœ¨ä½¿ç”¨ä¸Šè¿°ä»£ç æ—¶ç”±äºå„ç§åŸå› æ— æ³•ä» HuggingFace æ‹‰å–æ¨¡å‹å’Œä»£ç ï¼Œå¯ä»¥å…ˆä» ModelScope ä¸‹è½½æ¨¡å‹åŠä»£ç è‡³æœ¬åœ°ï¼Œå†ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼š

```python
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
# model_dir = snapshot_download('qwen/Qwen-VL')
model_dir = snapshot_download('qwen/Qwen-VL-Chat')


# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="cuda",
    trust_remote_code=True
).eval()
```

#### ğŸ¤– ModelScope

é­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from modelscope import (
    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig
)
import torch
model_id = 'qwen/Qwen-VL-Chat'
revision = 'v1.0.0'

model_dir = snapshot_download(model_id, revision=revision)
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
if not hasattr(tokenizer, 'model_dir'):
    tokenizer.model_dir = model_dir
# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True).eval()

# æŒ‡å®šç”Ÿæˆè¶…å‚æ•°ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰
# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
# Either a local path or an url between <img></img> tags.
image_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'
response, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)
print(response)
# å›¾ä¸­æ˜¯ä¸€åå¹´è½»å¥³å­åœ¨æ²™æ»©ä¸Šå’Œå¥¹çš„ç‹—ç©è€ï¼Œç‹—çš„å“ç§æ˜¯æ‹‰å¸ƒæ‹‰å¤šã€‚å¥¹ä»¬ååœ¨æ²™æ»©ä¸Šï¼Œç‹—çš„å‰è…¿æŠ¬èµ·æ¥ï¼Œä¸äººäº’åŠ¨ã€‚

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, 'è¾“å‡ºå‡»æŒçš„æ£€æµ‹æ¡†', history=history)
print(response)
# <ref>"å‡»æŒ"</ref><box>(211,412),(577,891)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('output_chat.jpg')
else:
  print("no box")
```

<br>

## é‡åŒ–

### ç”¨æ³•

å½“å‰æˆ‘ä»¬æä¾›äº†åŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†Qwen-VL-Chatçš„Int4é‡åŒ–ç‰ˆæœ¬Qwen-VL-Chat-Int4 [ç‚¹å‡»æ­¤å¤„](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)ã€‚è¯¥æ¨¡å‹åœ¨æ•ˆæœè¯„æµ‹ä¸Šå‡ ä¹æ— æŸï¼Œå¹¶åœ¨æ˜¾å­˜å ç”¨å’Œæ¨ç†é€Ÿåº¦ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

ä¸‹æ–‡è¯´æ˜å¦‚ä½•ä½¿ç”¨è¯¥é‡åŒ–æ¨¡å‹ã€‚å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch2.0åŠä»¥ä¸Šã€transformers 4.32.0åŠä»¥ä¸Šï¼Œç­‰ï¼‰å¹¶å®‰è£…æ‰€éœ€çš„ä»£ç åº“ï¼š

```bash
pip install optimum
git clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ
pip install -v .
```

å¦‚é‡åˆ°å®‰è£… `auto-gptq` çš„é—®é¢˜ï¼Œå»ºè®®æ‚¨å‰å¾€å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ) å¯»æ‰¾åˆé€‚çš„wheelã€‚

éšåä½ ä¾¿å¯ä»¥æŒ‰ç…§ä¸Šè¿°ç”¨æ³•****ï¼Œè½»æ¾è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š

```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
# Either a local path or an url between <img></img> tags.
image_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'
response, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)
print(response)
```

### æ•ˆæœè¯„æµ‹

æˆ‘ä»¬åˆ—å‡ºä¸åŒç²¾åº¦ä¸‹æ¨¡å‹åœ¨è¯„æµ‹åŸºå‡† **[TouchStone](https://github.com/OFA-Sys/TouchStone)** ä¸Šçš„è¡¨ç°ï¼Œå¹¶å‘ç°é‡åŒ–æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾è‘—æ€§èƒ½æŸå¤±ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization | ZH         | EN            |
| ------------ | :--------: | :-----------: | 
| BF16         | 401.2      |    645.2      |
| Int4         | 386.6      |    651.4      |

### æ¨ç†é€Ÿåº¦

æˆ‘ä»¬æµ‹ç®—äº†åœ¨è¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼ˆå³258ä¸ªtokenï¼‰çš„æ¡ä»¶ä¸‹BF16å’ŒInt4çš„æ¨¡å‹ç”Ÿæˆ1792 (2048-258) å’Œ 7934 (8192-258) ä¸ªtokençš„å¹³å‡é€Ÿåº¦ã€‚

| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |
| ------------ | :-----------------: | :-----------------: |
| BF16         |        28.87        |        24.32        |
| Int4         |        37.79        |        34.34        |

æ¨ç†é€Ÿåº¦æµ‹ç®—æ˜¯åœ¨å•å¡ A100-SXM4-80G GPUä¸Šè¿è¡Œï¼Œä½¿ç”¨PyTorch 2.0.1åŠCUDA 11.4ã€‚

### GPUæ˜¾å­˜å ç”¨

æˆ‘ä»¬è¿˜æµ‹ç®—äº†åœ¨ä¸€å¼ å›¾ç‰‡è¾“å…¥çš„æ¡ä»¶ä¸‹BF16å’ŒInt4æ¨¡å‹ç”Ÿæˆ1792 (2048-258) å’Œ 7934 (8192-258) ä¸ªtokenæ‰€éœ€æ˜¾å­˜ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |
| ------------ | :---------------------------------: | :-----------------------------------: |
| BF16         |               22.60GB               |                28.01GB                |
| Int4         |               11.82GB               |                17.23GB                |

ä¸Šè¿°é€Ÿåº¦å’Œæ˜¾å­˜æµ‹ç®—ä½¿ç”¨[æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)å®Œæˆã€‚
<br>

## å¾®è°ƒ

æˆ‘ä»¬æä¾›äº†`finetune.py`è¿™ä¸ªè„šæœ¬ä¾›ç”¨æˆ·å®ç°åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„åŠŸèƒ½ï¼Œä»¥æ¥å…¥ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†shellè„šæœ¬å‡å°‘ç”¨æˆ·çš„å·¥ä½œé‡ã€‚è¿™ä¸ªè„šæœ¬æ”¯æŒ [DeepSpeed](https://github.com/microsoft/DeepSpeed) å’Œ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) ã€‚æˆ‘ä»¬æä¾›çš„shellè„šæœ¬ä½¿ç”¨äº†DeepSpeedï¼Œå› æ­¤å»ºè®®æ‚¨ç¡®ä¿å·²ç»å®‰è£…DeepSpeedã€‚

é¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡ä½ çš„è®­ç»ƒæ•°æ®ã€‚ä½ éœ€è¦å°†æ‰€æœ‰æ ·æœ¬æ”¾åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­å¹¶å­˜å…¥jsonæ–‡ä»¶ä¸­ã€‚æ¯ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«idå’Œconversationï¼Œå…¶ä¸­åè€…ä¸ºä¸€ä¸ªåˆ—è¡¨ã€‚ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "ä½ å¥½"
      },
      {
        "from": "assistant",
        "value": "æˆ‘æ˜¯Qwen-VL,ä¸€ä¸ªæ”¯æŒè§†è§‰è¾“å…¥çš„å¤§æ¨¡å‹ã€‚"
      }
    ]
  },
  {
    "id": "identity_1",
    "conversations": [
      {
        "from": "user",
        "value": "Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\nå›¾ä¸­çš„ç‹—æ˜¯ä»€ä¹ˆå“ç§ï¼Ÿ"
      },
      {
        "from": "assistant",
        "value": "å›¾ä¸­æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"
      },
      {
        "from": "user",
        "value": "æ¡†å‡ºå›¾ä¸­çš„æ ¼å­è¡¬è¡«"
      },
      {
        "from": "assistant",
        "value": "<ref>æ ¼å­è¡¬è¡«</ref><box>(588,499),(725,789)</box>"
      }
    ]
  },
  { 
    "id": "identity_2",
    "conversations": [
      {
        "from": "user",
        "value": "Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\nå›¾ä¸­éƒ½æ˜¯å“ª"
      },
      {
        "from": "assistant",
        "value": "ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ã€‚"
      }
    ]
  }
]
```
ä¸ºé’ˆå¯¹å¤šæ ·çš„VLä»»åŠ¡ï¼Œæˆ‘ä»¬å¢åŠ äº†ä¸€ä¸‹çš„ç‰¹æ®Štokensï¼š `<img> </img> <ref> </ref> <box> </box>`.

å¯¹äºå¸¦å›¾åƒè¾“å…¥çš„å†…å®¹å¯è¡¨ç¤ºä¸º `Picture id: <img>img_path</img>\n{your prompt}`ï¼Œå…¶ä¸­`id`è¡¨ç¤ºå¯¹è¯ä¸­çš„ç¬¬å‡ å¼ å›¾ç‰‡ã€‚"img_path"å¯ä»¥æ˜¯æœ¬åœ°çš„å›¾ç‰‡æˆ–ç½‘ç»œåœ°å€ã€‚ 

å¯¹è¯ä¸­çš„æ£€æµ‹æ¡†å¯ä»¥è¡¨ç¤ºä¸º`<box>(x1,y1),(x2,y2)</box>`ï¼Œå…¶ä¸­ `(x1, y1)` å’Œ`(x2, y2)`åˆ†åˆ«å¯¹åº”å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡ï¼Œå¹¶ä¸”è¢«å½’ä¸€åŒ–åˆ°`[0, 1000)`çš„èŒƒå›´å†…. æ£€æµ‹æ¡†å¯¹åº”çš„æ–‡æœ¬æè¿°ä¹Ÿå¯ä»¥é€šè¿‡`<ref>text_caption</ref>`è¡¨ç¤ºã€‚


å‡†å¤‡å¥½æ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„shellè„šæœ¬å®ç°å¾®è°ƒã€‚æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šä½ çš„æ•°æ®çš„è·¯å¾„ã€‚

å¾®è°ƒè„šæœ¬èƒ½å¤Ÿå¸®ä½ å®ç°ï¼š
- å…¨å‚æ•°å¾®è°ƒ
- LoRA
- Q-LoRA

### å…¨å‚æ•°å¾®è°ƒ
é»˜è®¤ä¸‹å…¨å‚æ•°å¾®è°ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°LLMæ‰€æœ‰å‚æ•°ã€‚æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œåœ¨å¾®è°ƒé˜¶æ®µä¸æ›´æ–°ViTçš„å‚æ•°ä¼šå–å¾—æ›´å¥½çš„è¡¨ç°ã€‚ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªè„šæœ¬å¼€å§‹è®­ç»ƒï¼š

```bash
# åˆ†å¸ƒå¼è®­ç»ƒã€‚ç”±äºæ˜¾å­˜é™åˆ¶å°†å¯¼è‡´å•å¡è®­ç»ƒå¤±è´¥ï¼Œæˆ‘ä»¬ä¸æä¾›å•å¡è®­ç»ƒè„šæœ¬ã€‚
sh finetune/finetune_ds.sh
```

å°¤å…¶æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°æˆ–è·¯å¾„ã€æ•°æ®è·¯å¾„ã€ä»¥åŠæ¨¡å‹è¾“å‡ºçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚å¦‚æœä½ æƒ³ä¿®æ”¹deepspeedé…ç½®ï¼Œå¯ä»¥åˆ é™¤æ‰`--deepspeed`è¿™ä¸ªè¾“å…¥æˆ–è€…è‡ªè¡Œæ ¹æ®éœ€æ±‚ä¿®æ”¹DeepSpeedé…ç½®jsonæ–‡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå› æ­¤ä½ å¯ä»¥è®¾ç½®`--bf16 True`æˆ–è€…`--fp16 True`ã€‚ç»éªŒä¸Šï¼Œå¦‚æœä½ çš„æœºå™¨æ”¯æŒbf16ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨bf16ï¼Œè¿™æ ·å¯ä»¥å’Œæˆ‘ä»¬çš„é¢„è®­ç»ƒå’Œå¯¹é½è®­ç»ƒä¿æŒä¸€è‡´ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æŠŠé»˜è®¤é…ç½®è®¾ä¸ºå®ƒçš„åŸå› ã€‚

### LoRA
è¿è¡ŒLoRAçš„æ–¹æ³•ç±»ä¼¼å…¨å‚æ•°å¾®è°ƒã€‚ä½†åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…`peft`ä»£ç åº“ã€‚å¦å¤–ï¼Œè®°ä½è¦è®¾ç½®æ­£ç¡®çš„æ¨¡å‹ã€æ•°æ®å’Œè¾“å‡ºè·¯å¾„ã€‚æˆ‘ä»¬å»ºè®®ä½ ä¸ºæ¨¡å‹è·¯å¾„ä½¿ç”¨ç»å¯¹è·¯å¾„ã€‚è¿™æ˜¯å› ä¸ºLoRAä»…å­˜å‚¨adapteréƒ¨åˆ†å‚æ•°ï¼Œè€Œadapteré…ç½®jsonæ–‡ä»¶è®°å½•äº†é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œç”¨äºè¯»å–é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚åŒæ ·ï¼Œä½ å¯ä»¥è®¾ç½®bf16æˆ–è€…fp16ã€‚

```bash
# å•å¡è®­ç»ƒ
sh finetune/finetune_lora_single_gpu.sh
# åˆ†å¸ƒå¼è®­ç»ƒ
sh finetune/finetune_lora_ds.sh
```

ä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRA ([è®ºæ–‡](https://arxiv.org/abs/2106.09685)) åªæ›´æ–°adapterå±‚çš„å‚æ•°è€Œæ— éœ€æ›´æ–°åŸæœ‰è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•å…è®¸ç”¨æˆ·ç”¨æ›´ä½çš„æ˜¾å­˜å¼€é”€æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ„å‘³ç€æ›´å°çš„è®¡ç®—å¼€é”€ã€‚

æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œè€Œéchatæ¨¡å‹ï¼Œæ¨¡å‹çš„embeddingå’Œè¾“å‡ºå±‚çš„å‚æ•°å°†è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰å­¦ä¹ è¿‡ChatMLæ ¼å¼ä¸­çš„ç‰¹æ®Štokenï¼Œå› æ­¤éœ€è¦å°†è¿™éƒ¨åˆ†å‚æ•°è®¾ä¸ºå¯è®­ç»ƒæ‰èƒ½è®©æ¨¡å‹å­¦ä¼šç†è§£å’Œé¢„æµ‹è¿™äº›tokenã€‚è¿™ä¹Ÿæ„å‘³ç€ï¼Œå‡å¦‚ä½ çš„è®­ç»ƒå¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œä½ éœ€è¦é€šè¿‡ä»£ç ä¸­çš„`modules_to_save`å°†è¿™äº›å‚æ•°è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚å¦‚æœä½ æƒ³èŠ‚çœæ˜¾å­˜å ç”¨ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨chatæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œæ˜¾å­˜å ç”¨å°†å¤§å¹…åº¦é™ä½ã€‚ä¸‹æ–‡çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„è®°å½•å°†è¯¦ç»†ä»‹ç»è¿™éƒ¨åˆ†ç»†èŠ‚ã€‚

### Q-LoRA
å¦‚æœä½ ä¾ç„¶é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨Q-LoRA ([è®ºæ–‡](https://arxiv.org/abs/2305.14314))ã€‚è¯¥æ–¹æ³•ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹ä»¥åŠpaged attentionç­‰æŠ€æœ¯å®ç°æ›´å°çš„æ˜¾å­˜å¼€é”€ã€‚è¿è¡ŒQ-LoRAä½ åªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬ï¼š

```bash
# å•å¡è®­ç»ƒ
sh finetune/finetune_qlora_single_gpu.sh
# åˆ†å¸ƒå¼è®­ç»ƒ
sh finetune/finetune_qlora_ds.sh
```

æˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨æˆ‘ä»¬æä¾›çš„Int4é‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå³Qwen-VL-Chat-Int4ã€‚è¯·**ä¸è¦ä½¿ç”¨**éé‡åŒ–æ¨¡å‹ï¼ä¸å…¨å‚æ•°å¾®è°ƒä»¥åŠLoRAä¸åŒï¼ŒQ-LoRAä»…æ”¯æŒfp16ã€‚æ­¤å¤–ï¼Œä¸Šè¿°LoRAå…³äºç‰¹æ®Štokençš„é—®é¢˜åœ¨Q-LoRAä¾ç„¶å­˜åœ¨ã€‚å¹¶ä¸”ï¼ŒInt4æ¨¡å‹çš„å‚æ•°æ— æ³•è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚æ‰€å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬åªæä¾›äº†Chatæ¨¡å‹çš„Int4æ¨¡å‹ï¼Œå› æ­¤ä½ ä¸ç”¨æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ æ‰§æ„è¦åœ¨Q-LoRAä¸­å¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œå¾ˆæŠ±æ­‰ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯ä½ èƒ½æˆåŠŸè®­ç»ƒã€‚

ä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRAå’ŒQ-LoRAçš„è®­ç»ƒåªéœ€å­˜å‚¨adapteréƒ¨åˆ†çš„å‚æ•°ã€‚å‡å¦‚ä½ éœ€è¦ä½¿ç”¨LoRAè®­ç»ƒåçš„æ¨¡å‹ï¼Œä½ éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ã€‚ä½ å¯ä»¥ç”¨å¦‚ä¸‹ä»£ç è¯»å–æ¨¡å‹ï¼š

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
```

å¦‚æœä½ è§‰å¾—è¿™æ ·ä¸€æ­¥åˆ°ä½çš„æ–¹å¼è®©ä½ å¾ˆä¸å®‰å¿ƒæˆ–è€…å½±å“ä½ æ¥å…¥ä¸‹æ¸¸åº”ç”¨ï¼Œä½ å¯ä»¥é€‰æ‹©å…ˆåˆå¹¶å¹¶å­˜å‚¨æ¨¡å‹ï¼ˆLoRAæ”¯æŒåˆå¹¶ï¼ŒQ-LoRAä¸æ”¯æŒï¼‰ï¼Œå†ç”¨å¸¸è§„æ–¹å¼è¯»å–ä½ çš„æ–°æ¨¡å‹ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

æ³¨æ„ï¼šåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦æ ¹æ®ä½ çš„éœ€æ±‚å’Œæœºå™¨æŒ‡å®šæ­£ç¡®çš„åˆ†å¸ƒå¼è®­ç»ƒè¶…å‚æ•°ã€‚æ­¤å¤–ï¼Œä½ éœ€è¦æ ¹æ®ä½ çš„æ•°æ®ã€æ˜¾å­˜æƒ…å†µå’Œè®­ç»ƒé€Ÿåº¦é¢„æœŸï¼Œä½¿ç”¨`--model_max_length`è®¾å®šä½ çš„æ•°æ®é•¿åº¦ã€‚

### æ˜¾å­˜å ç”¨åŠè®­ç»ƒé€Ÿåº¦
ä¸‹é¢è®°å½•Qwen_VLæ¨¡å‹åœ¨å•GPUä½¿ç”¨LoRAï¼ˆLoRA (Base)æŒ‡çš„æ˜¯embeddingå’Œè¾“å‡ºå±‚å‚ä¸è®­ç»ƒï¼Œè€ŒLoRA (Chat)åˆ™ä¸ä¼˜åŒ–è¿™éƒ¨åˆ†å‚æ•°ï¼‰å’ŒQLoRAæ—¶å¤„ç†ä¸åŒé•¿åº¦è¾“å…¥çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æƒ…å†µã€‚æœ¬æ¬¡è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨CUDA 11.8å’ŒPytorch 2.0ã€‚æˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨batch sizeä¸º1ï¼Œgradient accumulationä¸º8çš„è®­ç»ƒé…ç½®ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€å¼ å›¾ï¼Œåˆ†åˆ«è®°å½•è¾“å…¥é•¿åº¦åˆ†åˆ«ä¸º384ã€512ã€1024å’Œ2048çš„æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰å’Œè®­ç»ƒé€Ÿåº¦ï¼ˆs/iterï¼‰ã€‚å…·ä½“æ•°å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

<table>
    <tr>
 <th rowspan="2">Method</th><th colspan="4" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">384</th><th align="center">512</th><th align="center">1024</th><th align="center">2048</th>
    </tr>
    <tr>
      <td>LoRA (Base)</td><td align="center">37.1G / 2.3s/it</td><td align="center">37.3G / 2.4s/it</td><td align="center">38.7G / 3.6s/it</td><td align="center">38.7G / 6.1s/it</td>
    </tr>
    <tr>
      <td>LoRA (Chat)</td><td align="center">23.3G / 2.2s/it</td><td align="center">23.6G / 2.3s/it</td><td align="center">25.1G / 3.5s/it</td><td align="center">27.3G / 5.9s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">17.0G / 4.2s/it</td><td align="center">17.2G / 4.5s/it</td><td align="center">18.2G / 5.5s/it</td><td align="center">19.3G / 7.9s/it</td>
    </tr>

</table>

<br><br>
## Demo

### Web UI

æˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š

```
pip install -r requirements_web_demo.txt
```

éšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š

```
python web_demo_mm.py
```

<br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜… [FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚
<br>

## ä½¿ç”¨åè®®

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwen-VLå’ŒQwen-VL-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚
<br>

## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„è®ºæ–‡å’Œä»£ç å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘:star: å’Œå¼•ç”¨ :pencil: :)

```BibTeX
@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
```
<br>

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

