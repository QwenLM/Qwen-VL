<br>

<p align="center">
    <img src="assets/logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-VL <a href="https://modelscope.cn/models/qwen/Qwen-VL/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-VL">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat <a href="https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-VL-Chat">ğŸ¤—</a>&nbsp ï½œ &nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://arxiv.org/pdf/2308.12966.pdf">Report</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp ï½œ &nbsp<a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/qwen_wechat_group.PNG">WeChat</a>

</p>
<br>

<p align="center">
        <a href="README_CN.md">ä¸­æ–‡</a>&nbsp ï½œ &nbsp <a href="README.md">English</a> ï½œ &nbsp æ—¥æœ¬èª
</p>
<br><br>
<p align="left">
        Japanese document maintainer: Ikko Eltociear Ashimine
</p>
<br>

**Qwen-VL** ï¼ˆQwen Large Vision Language Modelï¼‰ã¯ã€ã‚¢ãƒªãƒãƒã‚¯ãƒ©ã‚¦ãƒ‰ãŒæå”±ã™ã‚‹ãƒ©ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º Qwenï¼ˆç•¥ç§°: Tongyi Qianwenï¼‰ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç‰ˆã§ã™ã€‚Qwen-VL ã¯ã€ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘ä»˜ã‘ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚Qwen-VL ã®ç‰¹å¾´ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:
- **å¥½èª¿ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: è¤‡æ•°ã®è‹±èªè©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆZero-shot Captioningã€VQAã€DocVQAã€Grounding ã‚’å«ã‚€ï¼‰ã«ãŠã„ã¦ã€åŒæ§˜ã®ãƒ¢ãƒ‡ãƒ«è¦æ¨¡ã§ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚ŒãŸæ—¢å­˜ã®ãƒ©ãƒ¼ã‚¸ãƒ“ã‚¸ãƒ§ãƒ³è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLVLMï¼‰ã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã¾ã™ã€‚
- **ãƒ†ã‚­ã‚¹ãƒˆèªè­˜ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å¤šè¨€èª LVLM**: Qwen-VL ã¯ã€è‹±èªã€ä¸­å›½èªã€å¤šè¨€èªã®ä¼šè©±ã‚’è‡ªç„¶ã«ã‚µãƒãƒ¼ãƒˆã—ã€ç”»åƒå†…ã®ä¸­å›½èªã¨è‹±èªã®äºŒè¨€èªãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®èªè­˜ã‚’ä¿ƒé€²ã—ã¾ã™ã€‚
- **è¤‡æ•°ç”»åƒã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ä¼šè©±**: ã“ã®æ©Ÿèƒ½ã«ã‚ˆã‚Šã€è¤‡æ•°ã®ç”»åƒã‚’å…¥åŠ›ã—ã€æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ã¾ãŸã€ç”»åƒã«é–¢é€£ã™ã‚‹è³ªå•ã‚’æŒ‡å®šã—ã€è¤‡æ•°ã®ç”»åƒã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
- **ä¸­å›½èªã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ”¯ãˆã‚‹åˆã®ã‚¸ã‚§ãƒãƒ©ãƒªã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«**: ä¸­å›½èªã¨è‹±èªã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ³è¨€èªè¡¨ç¾ã«ã‚ˆã‚‹ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®æ¤œå‡ºã€‚
- **ãã‚ç´°ã‚„ã‹ãªèªè­˜ã¨ç†è§£**: ç¾åœ¨ä»–ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ LVLM ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ 224\*224 ã®è§£åƒåº¦ã¨æ¯”è¼ƒã—ã¦ã€448\*448 ã®è§£åƒåº¦ã¯ã€ãã‚ç´°ã‹ã„ãƒ†ã‚­ã‚¹ãƒˆèªè­˜ã€æ–‡æ›¸ QAã€ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æ³¨é‡ˆã‚’ä¿ƒé€²ã™ã‚‹ã€‚

<br>
<p align="center">
    <img src="assets/demo_vl.gif" width="400"/>
<p>
<br>

Qwen-VL ã‚·ãƒªãƒ¼ã‚ºã® 2 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¾ã™:
- Qwen-VL: LLM ã®åˆæœŸåŒ–ã« Qwen-7B ã‚’ã€è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®åˆæœŸåŒ–ã« [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) ã‚’ç”¨ã„ãŸå­¦ç¿’æ¸ˆã¿ LVLM ãƒ¢ãƒ‡ãƒ«ã€‚ãã—ã¦ã€ãã‚Œã‚‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æ¥ç¶šã™ã‚‹ã€‚
- Qwen-VL-Chat: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãª LLM ãƒ™ãƒ¼ã‚¹ã® AI ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€‚Qwen-VL-Chat ã¯ã€è¤‡æ•°ã®ç”»åƒå…¥åŠ›ã€è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®è³ªå•å¿œç­”ã€ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªæ©Ÿèƒ½ãªã©ã€ã‚ˆã‚ŠæŸ”è»Ÿãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚


## è©•ä¾¡

ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’2ã¤ã®è¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã—ã¾ã—ãŸ:
1. **æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¿ã‚¹ã‚¯ã®4ã¤ã®ä¸»è¦ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªã‚¿ã‚¹ã‚¯èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹:
   - ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³: æœªè¦‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹;
   - ä¸€èˆ¬çš„ãªVQA: åˆ¤å®šã€è‰²ã€æ•°ã€ã‚«ãƒ†ã‚´ãƒªãªã©ã€ç”»åƒã®ä¸€èˆ¬çš„ãªè³ªå•å¿œç­”èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹;
   - ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹VQA: æ–‡æ›¸QAã€å›³è¡¨QAãªã©ã€å†™çœŸå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èªè­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹;
   - å‚ç…§è¡¨ç¾ç†è§£: å‚ç…§è¡¨ç¾ç†è§£: å‚ç…§è¡¨ç¾ã§è¨˜è¿°ã•ã‚ŒãŸç”»åƒå†…ã®å¯¾è±¡ç‰©ã‚’ç‰¹å®šã™ã‚‹èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã€‚

2. **TouchStone**: ç·åˆçš„ãªãƒ†ã‚­ã‚¹ãƒˆç”»åƒå¯¾è©±èƒ½åŠ›ã¨äººé–“ã¨ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€GPT4 ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã«åŸºã¥ã TouchStone ã¨å‘¼ã°ã‚Œã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã€LVLM ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚
   - TouchStone ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€åˆè¨ˆ 300 ä»¥ä¸Šã®ç”»åƒã€800 ä»¥ä¸Šã®è³ªå•ã€27 ã®ã‚«ãƒ†ã‚´ãƒªã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€å±æ€§ãƒ™ãƒ¼ã‚¹ã® Q&Aã€æœ‰åäººã®èªè­˜ã€è©©ã®ä½œæ–‡ã€è¤‡æ•°ã®ç”»åƒã®è¦ç´„ã€å•†å“æ¯”è¼ƒã€æ•°å­¦ã®å•é¡Œè§£æ±ºãªã©ã§ã™;
   - ç”»åƒã®ç›´æ¥å…¥åŠ›ã¨ã„ã† GPT4 ã®ç¾åœ¨ã®åˆ¶é™ã‚’æ‰“ã¡ç ´ã‚‹ãŸã‚ã€TouchStone ã¯äººé–“ã®ãƒ©ãƒ™ãƒ«ä»˜ã‘ã«ã‚ˆã‚‹ãã‚ç´°ã‹ã„ç”»åƒæ³¨é‡ˆã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®è©³ç´°ãªæ³¨é‡ˆã¯ã€è³ªå•ã¨ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¨å…±ã«ã€æ¡ç‚¹ã®ãŸã‚ã« GPT4 ã«æç¤ºã•ã‚Œã¾ã™ã€‚
   - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã¯è‹±èªç‰ˆã¨ä¸­å›½èªç‰ˆãŒã‚ã‚Šã¾ã™ã€‚

è©•ä¾¡çµæœã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:

Qwen-VL ã¯ã€è¤‡æ•°ã® VL ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ç¾è¡Œã® SOTA ã‚¸ã‚§ãƒãƒ©ãƒªã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã€ã¾ãŸã€èƒ½åŠ› ç¯„å›²ã®ç‚¹ã§ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æŒã¡ã¾ã™ã€‚

<p align="center">
    <img src="assets/radar.png" width="600"/>
<p>

### ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¨ä¸€èˆ¬çš„ãª VQA
<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="2">Zero-shot Captioning</th>
    <th colspan="5">General VQA</th>
  </tr>
  <tr>
    <th>NoCaps</th>
    <th>Flickr30K</th>
    <th>VQAv2<sup>dev</sup></th>
    <th>OK-VQA</th>
    <th>GQA</th>
    <th>SciQA-Img<br>(0-shot)</th>
    <th>VizWiz<br>(0-shot)</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="10">Generalist<br>Models</td>
    <td>Flamingo-9B</td>
    <td>-</td>
    <td>61.5</td>
    <td>51.8</td>
    <td>44.7</td>
    <td>-</td>
    <td>-</td>
    <td>28.8</td>
  </tr>
  <tr>
    <td>Flamingo-80B</td>
    <td>-</td>
    <td>67.2</td>
    <td>56.3</td>
    <td>50.6</td>
    <td>-</td>
    <td>-</td>
    <td>31.6</td>
  </tr>
  <tr>
    <td>Unified-IO-XL</td>
    <td>100.0</td>
    <td>-</td>
    <td>77.9</td>
    <td>54.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Kosmos-1</td>
    <td>-</td>
    <td>67.1</td>
    <td>51.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>29.2</td>
  </tr>
  <tr>
    <td>Kosmos-2</td>
    <td>-</td>
    <td>80.5</td>
    <td>51.1</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>103.9</td>
    <td>71.6</td>
    <td>65.0</td>
    <td>45.9</td>
    <td>32.3</td>
    <td>61.0</td>
    <td>19.6</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td><strong>121.9</strong></td>
    <td>82.8</td>
    <td>-</td>
    <td>-</td>
    <td>49.5</td>
    <td>63.1</td>
    <td>33.4</td>
  </tr>
  <tr>
    <td>Shikra (Vicuna-13B)</td>
    <td>-</td>
    <td>73.9</td>
    <td>77.36</td>
    <td>47.16</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td><strong>Qwen-VL (Qwen-7B)</strong></td>
    <td>121.4</td>
    <td><b>85.8</b></td>
    <td><b>78.8</b></td>
    <td><b>58.6</b></td>
    <td><b>59.3</b></td>
    <td>67.1</td>
    <td>35.2</td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>63.6</td>
    <td>-</td>
    <td>-</td>
    <td>39.1</td>
  </tr> -->
  <tr>
    <td>Qwen-VL-Chat</td>
    <td>120.2</td>
    <td>81.0</td>
    <td>78.2</td>
    <td>56.6</td>
    <td>57.5</td>
    <td><b>68.2</b></td>
    <td><b>38.9</b></td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL-Chat (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>60.6</td>
    <td>-</td>
    <td>-</td>
    <td>44.45</td>
  </tr> -->
  <tr>
    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>
    <td>-</td>
    <td>127.0<br>(PALI-17B)</td>
    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>
    <td>86.1<br>(PALI-X<br>-55B)</td>
    <td>66.1<br>(PALI-X<br>-55B)</td>
    <td>72.1<br>(CFR)</td>
    <td>92.53<br>(LLaVa+<br>GPT-4)</td>
    <td>70.9<br>(PALI-X<br>-55B)</td>
  </tr>
</tbody>
</table>

- ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ä»˜ã‘ã§ã¯ã€Qwen-VL ã¯ Flickr30K ã§ **SOTA** ã‚’é”æˆã—ã€InstructBlip ã‚’ä½¿ç”¨ã—ãŸ Nocaps ã§ã‚‚ç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’å¾—ã¦ã„ã¾ã™ã€‚
- ä¸€èˆ¬çš„ãª VQA ã§ã¯ã€Qwen-VL ã¯åŒã˜ä¸€èˆ¬çš„ãª LVLM ã‚¹ã‚±ãƒ¼ãƒ«è¨­å®šã§ **SOTA** ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

### ãƒ†ã‚­ã‚¹ãƒˆæŒ‡å‘VQAï¼ˆç”»åƒä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆç†è§£èƒ½åŠ›ã«é‡ç‚¹ã‚’ç½®ãï¼‰

<table>
<thead>
  <tr>
    <th>Model type</th>
    <th>Model</th>
    <th>TextVQA</th>
    <th>DocVQA</th>
    <th>ChartQA</th>
    <th>AI2D</th>
    <th>OCR-VQA</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="5">Generalist Models</td>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>42.4</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td>50.7</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>mPLUG-DocOwl (LLaMA-7B)</td>
    <td>52.6</td>
    <td>62.2</td>
    <td>57.4</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Pix2Struct-Large (1.3B)</td>
    <td>-</td>
    <td><b>76.6</b></td>
    <td>58.6</td>
    <td>42.1</td>
    <td>71.3</td>
  </tr>
  <tr>
    <td>Qwen-VL (Qwen-7B)</td>
    <td><b>63.8</b></td>
    <td>65.1</td>
    <td><b>65.7</b></td>
    <td><b>62.3</b></td>
    <td><b>75.7</b></td>
  </tr>
  <tr>
    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>
    <td>71.44</td>
    <td>80.0</td>
    <td>70.0</td>
    <td>81.2</td>
    <td>75.0</td>
  </tr>
</tbody>
</table>

- ãƒ†ã‚­ã‚¹ãƒˆé–¢é€£ã®èªè­˜/QA è©•ä¾¡ã«ãŠã„ã¦ã€Qwen-VL ã¯æ±ç”¨ã® LVLM ã‚¹ã‚±ãƒ¼ãƒ«è¨­å®šã§ SOTA ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚
- è§£åƒåº¦ã¯ä¸Šè¨˜ã®ã„ãã¤ã‹ã®è©•ä¾¡ã«ãŠã„ã¦é‡è¦ã§ã‚ã‚‹ã€‚è§£åƒåº¦ãŒ 224 ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã® LVLM ãƒ¢ãƒ‡ãƒ«ã®å¤šãã¯ã€ã“ã‚Œã‚‰ã®è©•ä¾¡ãŒã§ããªã„ã‹ã€ç”»åƒã‚’ã‚«ãƒƒãƒˆã™ã‚‹ã“ã¨ã§ã—ã‹è§£æ±ºã§ããªã„ãŒã€Qwen-VL ã¯è§£åƒåº¦ã‚’ 448 ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§è©•ä¾¡ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚Qwen-VL ã¯ã€ä¸€éƒ¨ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€è§£åƒåº¦ 1024 ã® Pix2Struct-Large ãƒ¢ãƒ‡ãƒ«ã‚’ã‚‚å‡Œé§•ã—ã¦ã„ã¾ã™ã€‚

### è¡¨ç¾ç†è§£ã®å‚ç…§
<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="3">RefCOCO</th>
    <th colspan="3">RefCOCO+</th>
    <th colspan="2">RefCOCOg</th>
    <th>GRIT</th>
  </tr>
  <tr>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val-u</th>
    <th>test-u</th>
    <th>refexp</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="8">Generalist Models</td>
    <td>GPV-2</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>51.50</td>
  </tr>
  <tr>
    <td>OFA-L*</td>
    <td>79.96</td>
    <td>83.67</td>
    <td>76.39</td>
    <td>68.29</td>
    <td>76.00</td>
    <td>61.75</td>
    <td>67.57</td>
    <td>67.58</td>
    <td>61.70</td>
  </tr>
  <tr>
    <td>Unified-IO</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td><b>78.61</b></td>
  </tr>
  <tr>
    <td>VisionLLM-H</td>
    <td></td>
    <td>86.70</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Shikra-7B</td>
    <td>87.01</td>
    <td>90.61</td>
    <td>80.24 </td>
    <td>81.60</td>
    <td>87.36</td>
    <td>72.12</td>
    <td>82.27</td>
    <td>82.19</td>
    <td>69.34</td>
  </tr>
  <tr>
    <td>Shikra-13B</td>
    <td>87.83 </td>
    <td>91.11</td>
    <td>81.81</td>
    <td>82.89</td>
    <td>87.79</td>
    <td>74.41</td>
    <td>82.64</td>
    <td>83.16</td>
    <td>69.03</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B</td>
    <td><b>89.36</b></td>
    <td>92.26</td>
    <td><b>85.34</b></td>
    <td><b>83.12</b></td>
    <td>88.25</td>
    <td><b>77.21</b></td>
    <td>85.58</td>
    <td>85.48</td>
    <td>78.22</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B-Chat</td>
    <td>88.55</td>
    <td><b>92.27</b></td>
    <td>84.51</td>
    <td>82.82</td>
    <td><b>88.59</b></td>
    <td>76.79</td>
    <td><b>85.96</b></td>
    <td><b>86.32</b></td>
    <td>-</td>
  <tr>
    <td rowspan="3">Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>G-DINO-L</td>
    <td>90.56&nbsp;&nbsp;</td>
    <td>93.19</td>
    <td>88.24</td>
    <td>82.75</td>
    <td>88.95</td>
    <td>75.92</td>
    <td>86.13</td>
    <td>87.02</td>
    <td>-</td>
  </tr>
  <tr>
    <td>UNINEXT-H</td>
    <td>92.64 </td>
    <td>94.33</td>
    <td>91.46</td>
    <td>85.24</td>
    <td>89.63</td>
    <td>79.79</td>
    <td>88.73</td>
    <td>89.37</td>
    <td>-</td>
  </tr>
  <tr>
    <td>ONE-PEACE</td>
    <td>92.58 </td>
    <td>94.18</td>
    <td>89.26</td>
    <td>88.77</td>
    <td>92.21</td>
    <td>83.23</td>
    <td>89.22</td>
    <td>89.27</td>
    <td>-</td>
  </tr>
</tbody>
</table>

- Qwen-VL ã¯ã€ä¸Šè¨˜ã®ã™ã¹ã¦ã®å‚ç…§è¡¨ç¾ç†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ **SOTA** ã‚’é”æˆã—ãŸã€‚
- Qwen-VL ã¯ä¸­å›½èªã®ä¸‹åœ°ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã¦ã„ãªã„ãŒã€ä¸­å›½èªã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨è‹±èªã®ä¸‹åœ°ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ä¸­å›½èªã®ä¸‹åœ°ã‚¿ã‚¹ã‚¯ã«æ±åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ç§ãŸã¡ã®å®Ÿé¨“çµæœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€ä¸Šè¨˜ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã™ã¹ã¦æä¾›ã—ã¦ã„ã¾ã™ã€‚è©³ã—ãã¯ [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) ã‚’ãŠèª­ã¿ãã ã•ã„ã€‚

### ãƒãƒ£ãƒƒãƒˆè©•ä¾¡

TouchStone ã¯ GPT4 ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã«åŸºã¥ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®å¯¾è©±ãŠã‚ˆã³äººé–“ã¨ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã«ãŠã‘ã‚‹ LVLM ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã€‚åˆè¨ˆ 300 ä»¥ä¸Šã®ç”»åƒã€800 ä»¥ä¸Šã®è³ªå•ã€å±æ€§ãƒ™ãƒ¼ã‚¹ã® Q&Aã€æœ‰åäººã®èªè­˜ã€è©©ã®ä½œæˆã€è¤‡æ•°ã®ç”»åƒã®è¦ç´„ã€å•†å“æ¯”è¼ƒã€æ•°å­¦ã®å•é¡Œè§£æ±ºãªã©27ã®ã‚«ãƒ†ã‚´ãƒªã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚è©³ã—ãã¯ [touchstone/README_JA.md](touchstone/README_JA.md) ã‚’ãŠèª­ã¿ãã ã•ã„ã€‚

#### è‹±èªè©•ä¾¡

| Model           | Score |
| --------------- | ----- |
| PandaGPT        | 488.5 |
| MiniGPT4        | 531.7 |
| InstructBLIP    | 552.4 |
| LLaMA-AdapterV2 | 590.1 |
| LLaVA           | 602.7 |
| mPLUG-Owl       | 605.4 |
| Qwen-VL-Chat    | 645.2 |

#### ä¸­å›½èªè©•ä¾¡

| Model        | Score |
| ------------ | ----- |
| VisualGLM    | 247.1 |
| Qwen-VL-Chat | 401.2 |

Qwen-VL-Chat ã¯ä¸­å›½èªã¨è‹±èªã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆè©•ä¾¡ã§æœ€é«˜ã®çµæœã‚’å¾—ã¾ã—ãŸã€‚

## å¿…è¦æ¡ä»¶

* python 3.8 ä»¥ä¸Š
* pytorch 1.12 ä»¥ä¸Šã€2.0 ä»¥ä¸Šã‚’æ¨å¥¨
* CUDA 11.4 ä»¥ä¸Šã‚’æ¨å¥¨ï¼ˆGPU ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã§ã™ï¼‰

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

ä»¥ä¸‹ã§ã¯ã€Qwen-VL ã¨ Qwen-VL-Chat ã‚’ ğŸ¤– ModelScope ã¨ ğŸ¤— Transformers ã¨ã¨ã‚‚ã«ä½¿ã†æ–¹æ³•ã‚’ã€ç°¡å˜ãªä¾‹ã§ç¤ºã—ã¾ã™ã€‚

ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒæ¸ˆã‚“ã§ã„ã‚‹ã“ã¨ã‚’ ç¢ºèªã—ã¦ãã ã•ã„ã€‚ä¸Šè¨˜ã®è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã€ä¾å­˜ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

```bash
pip install -r requirements.txt
```

ã“ã‚Œã§ ModelScope ã‚„ Transformers ã‚’ä½¿ã„å§‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã¤ã„ã¦ã®è©³ã—ã„ä½¿ã„æ–¹ã¯ã€[ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](TUTORIAL.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

#### ğŸ¤— Transformers

Qwen-VL-Chat ã‚’æ¨è«–ã«ä½¿ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã®ã¯ã€ä»¥ä¸‹ã«ç¤ºã™æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ã ã‘ã§ã™ã€‚ãŸã ã—ã€**æœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# Note: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‹•ä½œã§ã¯ã€ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒé˜²æ­¢æ©Ÿèƒ½ãŒã‚ªãƒ•ã«ãªã‚Šã¾ã—ãŸã€‚
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# bf16 ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# fp16 ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# cpu ã®ã¿ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cpu", trust_remote_code=True).eval()
# cuda ãƒ‡ãƒã‚¤ã‚¹ã®ä½¿ç”¨
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda", trust_remote_code=True).eval()

# ç”Ÿæˆã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŒ‡å®š
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# ç¬¬ 1 å› å¯¾è©±ã‚¿ãƒ¼ãƒ³
query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ url
    {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# å†™çœŸã¯ãƒ“ãƒ¼ãƒã§ãƒ©ãƒ–ãƒ©ãƒ‰ãƒ¼ãƒ«ã®éš£ã§æ„›çŠ¬ã¨æˆ¯ã‚Œã‚‹å¥³æ€§ãŒå†™ã£ã¦ãŠã‚Šã€å½¼ã‚‰ã¯ç ‚ã®ä¸­ã«ã„ã‚‹ã€‚

# ç¬¬ 2 å› å¯¾è©±ã‚¿ãƒ¼ãƒ³
response, history = model.chat(tokenizer, 'æ¡†å‡ºå›¾ä¸­å‡»æŒçš„ä½ç½®', history=history)
print(response)
# <ref>å‡»æŒ</ref><box>(536,509),(588,602)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('1.jpg')
else:
  print("no box")
```

<p align="center">
    <img src="assets/demo_highfive.jpg" width="500"/>
<p>

<details>
  <summary>Running Qwen-VL</summary>

Running Qwen-VL pretrained base model is also simple.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL", trust_remote_code=True)

# bf16 ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="auto", trust_remote_code=True, bf16=True).eval()
# fp16 ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="auto", trust_remote_code=True, fp16=True).eval()
# cpu ã®ã¿ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="cpu", trust_remote_code=True).eval()
# cuda ãƒ‡ãƒã‚¤ã‚¹ã®ä½¿ç”¨
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL", device_map="cuda", trust_remote_code=True).eval()

# ç”Ÿæˆã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŒ‡å®š
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL", trust_remote_code=True)

query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ url
    {'text': 'Generate the caption in English with grounding:'},
])
inputs = tokenizer(query, return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)
print(response)
# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>
image = tokenizer.draw_bbox_on_latest_picture(response)
if image:
  image.save('2.jpg')
else:
  print("no box")
```

<p align="center">
    <img src="assets/demo_spotting_caption.jpg" width="500"/>
<p>

</details>


#### ğŸ¤– ModelScope

ModelScope ã¯ã€MaaSï¼ˆModel-as-a-Serviceï¼‰ã®ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚ã‚Šã€AI é–‹ç™ºè€…ã«æŸ”è»Ÿã§è²»ç”¨å¯¾åŠ¹æœã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚åŒæ§˜ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ã« ModelScope ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™:

```python
from modelscope import (
    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig
)
import torch
model_id = 'qwen/Qwen-VL-Chat'
revision = 'v1.0.0'

model_dir = snapshot_download(model_id, revision=revision)
torch.manual_seed(1234)

tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
if not hasattr(tokenizer, 'model_dir'):
    tokenizer.model_dir = model_dir
# bf16 ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, bf16=True).eval()
# fp16 ã®ä½¿ç”¨
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, fp16=True).eval()
# cpu ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="cpu", trust_remote_code=True).eval()
# auto ã®ä½¿ç”¨
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True).eval()

# ç”Ÿæˆã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŒ‡å®š
model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)

# ç¬¬ 1 å› å¯¾è©±ã‚¿ãƒ¼ãƒ³
# Either a local path or an url between <img></img> tags.
image_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'
response, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)
print(response)
# å†™çœŸã¯ã€è‹¥ã„å¥³æ€§ãŒãƒ“ãƒ¼ãƒã§æ„›çŠ¬ã®ãƒ©ãƒ–ãƒ©ãƒ‰ãƒ¼ãƒ«ç¨®ã¨æˆ¯ã‚Œã¦ã„ã‚‹ã¨ã“ã‚ã€‚ äºŒäººã¯æµœè¾ºã«åº§ã‚Šã€çŠ¬ã®å‰è„šã‚’ä¸Šã’ã¦è§¦ã‚Œåˆã£ã¦ã„ã‚‹ã€‚

# ç¬¬ 2 å› å¯¾è©±ã‚¿ãƒ¼ãƒ³
response, history = model.chat(tokenizer, 'è¾“å‡ºå‡»æŒçš„æ£€æµ‹æ¡†', history=history)
print(response)
# <ref>"å‡»æŒ"</ref><box>(211,412),(577,891)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
  image.save('output_chat.jpg')
else:
  print("no box")
```

<p align="center">
    <img src="assets/demo_highfive.jpg" width="500"/>
<p>

## ãƒ‡ãƒ¢

### Web UI

Web UI ãƒ‡ãƒ¢ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚å§‹ã‚ã‚‹å‰ã«ã€ä»¥ä¸‹ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„:

```bash
pip install -r requirements_web_demo.txt
```

æ¬¡ã«ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€ç”Ÿæˆã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™:

```bash
python web_demo_mm.py
```

## FAQ

å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€[FAQ](FAQ_ja.md) ã‚„ issue ã‚’å‚ç…§ã—ã€æ–°ã—ã„ issue ã‚’ç«‹ã¡ä¸Šã’ã‚‹å‰ã«è§£æ±ºç­–ã‚’æ¢ã—ã¦ãã ã•ã„ã€‚


## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å¥‘ç´„

ç ”ç©¶è€…ã‚„é–‹ç™ºè€…ã¯ã€Qwen-VL ã¨ Qwen-VL-Chat ã®ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚’è‡ªç”±ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€å•†ç”¨åˆ©ç”¨ã‚‚å¯èƒ½ã§ã™ã€‚è©³ã—ãã¯ [LICENSE](LICENSE) ã‚’ã”è¦§ãã ã•ã„ã€‚

## Citation

If you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)

```BibTeX
@article{Qwen-VL,
  title={Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
```

## ãŠå•ã„åˆã‚ã›

ç ”ç©¶ãƒãƒ¼ãƒ ã¾ãŸã¯è£½å“ãƒãƒ¼ãƒ ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã€qianwen_opensource@alibabacloud.com ã¾ã§ãŠæ°—è»½ã«ãŠé€ã‚Šãã ã•ã„ã€‚

